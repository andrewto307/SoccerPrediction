{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:30px;'>Data Featuring</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2 style='font-size:25px;'>Import</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = pd.read_csv(\"../data/X_train.csv\", index_col=0)\n",
    "X_test_full  = pd.read_csv(\"../data/X_test.csv\", index_col=0)\n",
    "\n",
    "y_train = pd.read_csv(\"../data/y_train.csv\", index_col=0).squeeze()\n",
    "y_test  = pd.read_csv(\"../data/y_test.csv\", index_col=0).squeeze()\n",
    "\n",
    "# Make stripped-down versions for the model\n",
    "non_features = [\"Div\", \"Date\", \n",
    "                \"HomeTeam_ShotOnTarget\", \"AwayTeam_ShotOnTarget\"]\n",
    "\n",
    "X_train_features = X_train_full.drop(columns=non_features)\n",
    "X_test_features  = X_test_full.drop(columns=non_features)\n",
    "\n",
    "# Save the feature order from the stripped-down version\n",
    "feature_columns = X_train_features.columns\n",
    "feature_columns.to_series(name=\"feature\").to_csv(\"../data/feature_columns.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feat_order = pd.read_csv(\"../data/feature_columns.csv\")[\"feature\"].tolist()\n",
    "\n",
    "def align_features(df: pd.DataFrame, feat_order: list[str]) -> pd.DataFrame:\n",
    "    # add any missing training columns as zeros\n",
    "    missing = [c for c in feat_order if c not in df.columns]\n",
    "    if missing:\n",
    "        df = df.copy()\n",
    "        for c in missing:\n",
    "            df[c] = 0.0\n",
    "\n",
    "    # drop any extras not used in training\n",
    "    extra = [c for c in df.columns if c not in feat_order]\n",
    "    if extra:\n",
    "        df = df.drop(columns=extra, errors=\"ignore\")\n",
    "\n",
    "    # put in the exact training order\n",
    "    df = df.reindex(columns=feat_order, fill_value=0.0)\n",
    "    return df\n",
    "\n",
    "# build the actual matrices the model will see\n",
    "X_train = align_features(X_train_features, feat_order)\n",
    "X_test  = align_features(X_test_features,  feat_order)\n",
    "\n",
    "# optional safety check\n",
    "assert list(X_train.columns) == feat_order == list(X_test.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2 style='font-size:25px;'>Probabilisitc Data Selection</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       group     brier\n",
      "0  consensus  0.557498\n",
      "6    odds_WH  0.583697\n",
      "8   odds_PSC  0.584901\n",
      "7    odds_VC  0.585204\n",
      "4    odds_BW  0.587706\n",
      "5    odds_IW  0.589967\n",
      "1  odds_B365  0.863573\n",
      "2    odds_PS  0.865276\n",
      "3   odds_Max  0.872169\n"
     ]
    }
   ],
   "source": [
    "def brier_score_multi(p, y):\n",
    "    Y = pd.get_dummies(y).reindex(columns=[0,1,2], fill_value=0).values\n",
    "    return float(np.mean(np.sum((p - Y)**2, axis=1)))\n",
    "\n",
    "def group_brier_scores(X, y):\n",
    "    rows = []\n",
    "    # consensus: probabilities already\n",
    "    if set(['pH_mean','pD_mean','pA_mean']).issubset(X.columns):\n",
    "        P = X[['pA_mean','pD_mean','pH_mean']].values  # order: 0,1,2\n",
    "        P = np.clip(P, 1e-6, 1-1e-6)\n",
    "        P = P / P.sum(axis=1, keepdims=True)\n",
    "        rows.append({'group':'consensus', 'brier': brier_score_multi(P, y)})\n",
    "    # raw odds approximate probabilities via 1/odds then renormalize\n",
    "    triplets = [('B365A','B365D','B365H'), ('PSA','PSD','PSH'), ('MaxA','MaxD','MaxH'), (\"BWH\",\"BWD\",\"BWA\"), ( \"IWH\",\"IWD\",\"IWA\"), \n",
    "                (\"WHH\",\"WHD\",\"WHA\"), (\"VCH\",\"VCD\",\"VCA\"), (\"PSCH\",\"PSCD\",\"PSCA\")]\n",
    "    for nameA,nameD,nameH in triplets:\n",
    "        if {nameA,nameD,nameH}.issubset(X.columns):\n",
    "            P = 1.0 / X[[nameA,nameD,nameH]].values\n",
    "            P = np.clip(P, 1e-6, None)\n",
    "            P = P / P.sum(axis=1, keepdims=True)\n",
    "            rows.append({'group': f'odds_{nameH[:-1]}', 'brier': brier_score_multi(P, y)})\n",
    "    return pd.DataFrame(rows).sort_values('brier')\n",
    "\n",
    "brier_df = group_brier_scores(X_train, y_train)\n",
    "print(brier_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop bad odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = [\"B365H\",\"B365D\",\"B365A\", \"PSH\",\"PSD\",\"PSA\", \"MaxH\",\"MaxD\",\"MaxA\"]\n",
    "X_train = X_train.drop(columns=exclude)\n",
    "X_test = X_test.drop(columns=exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2 style='font-size:25px;'>Data Selection</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Define feature groups ----\n",
    "\n",
    "# Form features\n",
    "form_features = [\n",
    "    \"HomeTeam_points\", \"AwayTeam_points\",\n",
    "    \"HomeTeam_avg_goal_diff\", \"AwayTeam_avg_goal_diff\"\n",
    "]\n",
    "\n",
    "# Raw bookmaker odds (all H/D/A triplets)\n",
    "bookmaker_odds = [\n",
    "    \"BWH\",\"BWD\",\"BWA\",\n",
    "    \"IWH\",\"IWD\",\"IWA\",\n",
    "    \"WHH\",\"WHD\",\"WHA\",\n",
    "    \"VCH\",\"VCD\",\"VCA\",\n",
    "    \"PSCH\",\"PSCD\",\"PSCA\"\n",
    "]\n",
    "\n",
    "# Overrounds (one per bookmaker set)\n",
    "overrounds = [\n",
    "    \"B365_overround\",\"BW_overround\",\"IW_overround\",\"WH_overround\",\n",
    "    \"VC_overround\",\"Max_overround\",\"PS_overround\",\"PSC_overround\"\n",
    "]\n",
    "\n",
    "# Elo ratings\n",
    "elo_features = [\"home_elo\",\"away_elo\",\"elo_diff\"]\n",
    "\n",
    "# Consensus features\n",
    "consensus_features = [\"pH_mean\",\"pD_mean\",\"pA_mean\",\"overround_mean\",\"overround_std\"]\n",
    "\n",
    "# Engineered extras\n",
    "engineered_features = [\"home_adv\",\"draw_tightness\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3 style='font-size:20px;'>Informative groups</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            group  features   mi_mean  mi_median  mi_top3_sum\n",
      "1  bookmaker_odds        15  0.079875   0.100196     0.346286\n",
      "4       consensus         3  0.080426   0.107707     0.241277\n",
      "3             elo         3  0.059961   0.048379     0.179883\n",
      "5      engineered         2  0.065186   0.065186     0.130372\n",
      "0            form         4  0.023051   0.024665     0.084605\n",
      "2      overrounds         8  0.011276   0.011363     0.063325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def group_mutual_info(X, y, groups):\n",
    "    scores = []\n",
    "    for gname, feats in groups.items():\n",
    "        feats = [f for f in feats if f in X.columns]\n",
    "        if not feats:\n",
    "            continue\n",
    "        mi = mutual_info_classif(X[feats], y, random_state=42, discrete_features=False)\n",
    "        scores.append({\n",
    "            'group': gname,\n",
    "            'features': len(feats),\n",
    "            'mi_mean': float(np.mean(mi)),\n",
    "            'mi_median': float(np.median(mi)),\n",
    "            'mi_top3_sum': float(np.sum(np.sort(mi)[-3:])),\n",
    "        })\n",
    "    return pd.DataFrame(scores).sort_values(['mi_top3_sum','mi_mean'], ascending=False)\n",
    "\n",
    "groups = {\n",
    "    'form': form_features,\n",
    "    'bookmaker_odds': bookmaker_odds,\n",
    "    'overrounds': overrounds,\n",
    "    'elo': elo_features,\n",
    "    'consensus': consensus_features,\n",
    "    'engineered': engineered_features\n",
    "}\n",
    "\n",
    "mi_df = group_mutual_info(X_train, y_train, groups)\n",
    "print(mi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3 style='font-size:20px;'>Group redundancy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            group  intragroup_corr_mean\n",
      "0            form              0.309717\n",
      "1  bookmaker_odds              0.659672\n",
      "2      overrounds              0.528862\n",
      "3             elo              0.474944\n",
      "4       consensus              0.531874\n",
      "5      engineered              0.330012\n"
     ]
    }
   ],
   "source": [
    "def group_redundancy(X, groups):\n",
    "    rows = []\n",
    "    for gname, feats in groups.items():\n",
    "        feats = [f for f in feats if f in X.columns]\n",
    "        if len(feats) < 2:\n",
    "            rows.append({'group': gname, 'intragroup_corr_mean': 0.0})\n",
    "            continue\n",
    "        corr = X[feats].corr().abs()\n",
    "        upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "        mean_corr = upper.stack().mean() if upper.size else 0.0\n",
    "        rows.append({'group': gname, 'intragroup_corr_mean': float(mean_corr)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "redundancy_df = group_redundancy(X_train, groups)\n",
    "print(redundancy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3 style='font-size:20px;'>Temporal Stability</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "form: features=4, used=4, psi_mean=0.0149\n",
      "bookmaker_odds: features=15, used=15, psi_mean=0.0750\n",
      "overrounds: features=8, used=8, psi_mean=4.3166\n",
      "elo: features=3, used=3, psi_mean=0.1937\n",
      "consensus: features=3, used=3, psi_mean=0.0850\n",
      "engineered: features=2, used=2, psi_mean=0.1131\n",
      "            group  psi_mean  features  used\n",
      "0            form  0.014941         4     4\n",
      "1  bookmaker_odds  0.075031        15    15\n",
      "4       consensus  0.084992         3     3\n",
      "5      engineered  0.113055         2     2\n",
      "3             elo  0.193702         3     3\n",
      "2      overrounds  4.316615         8     8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def psi_with_common_bins(a, b, bins=10):\n",
    "    a = pd.Series(a, dtype='float64').dropna()\n",
    "    b = pd.Series(b, dtype='float64').dropna()\n",
    "    if len(a) < 20 or len(b) < 20:\n",
    "        return 0.0\n",
    "    s = pd.concat([a, b])\n",
    "    if s.nunique(dropna=True) <= 2 or s.std() == 0:\n",
    "        return 0.0\n",
    "\n",
    "    qs = np.linspace(0, 1, bins + 1)\n",
    "    edges = np.unique(np.nanquantile(s, qs))\n",
    "    if len(edges) < 3:\n",
    "        return 0.0\n",
    "\n",
    "    ca = pd.cut(a, edges, include_lowest=True)\n",
    "    cb = pd.cut(b, edges, include_lowest=True)\n",
    "\n",
    "    pa = ca.value_counts(normalize=True, sort=False)\n",
    "    pb = cb.value_counts(normalize=True, sort=False)\n",
    "\n",
    "    idx = pa.index.union(pb.index)\n",
    "    pa = pa.reindex(idx, fill_value=0).astype('float64') + 1e-6\n",
    "    pb = pb.reindex(idx, fill_value=0).astype('float64') + 1e-6\n",
    "\n",
    "    return float(np.sum((pa - pb) * np.log(pa / pb)))\n",
    "\n",
    "def group_stability_no_date_common_bins(X, groups, split_ratio=0.7, bins=10, verbose=False):\n",
    "    n = len(X)\n",
    "    cut = int(n * split_ratio)\n",
    "    early, late = X.iloc[:cut], X.iloc[cut:]\n",
    "\n",
    "    # keep only numeric columns once to avoid repeated inference\n",
    "    numeric_cols = set(X.select_dtypes(include=[np.number]).columns)\n",
    "\n",
    "    rows = []\n",
    "    for gname, feats in groups.items():\n",
    "        feats = [f for f in feats if f in X.columns]\n",
    "        feats = [f for f in feats if f in numeric_cols]  # ensure numeric\n",
    "        psis, used = [], 0\n",
    "\n",
    "        for f in feats:\n",
    "            a = pd.to_numeric(early[f], errors='coerce')\n",
    "            b = pd.to_numeric(late[f], errors='coerce')\n",
    "            if a.dropna().empty or b.dropna().empty:\n",
    "                continue\n",
    "            try:\n",
    "                psis.append(psi_with_common_bins(a.values, b.values, bins=bins))\n",
    "                used += 1\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        psi_mean = float(np.mean(psis)) if used > 0 else 0.0  # default to 0 (stable) if none usable\n",
    "        if verbose:\n",
    "            print(f\"{gname}: features={len(feats)}, used={used}, psi_mean={psi_mean:.4f}\")\n",
    "\n",
    "        rows.append({'group': gname, 'psi_mean': psi_mean, 'features': len(feats), 'used': used})\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values('psi_mean')\n",
    "\n",
    "# Usage\n",
    "groups = {\n",
    "    'form': form_features,\n",
    "    'bookmaker_odds': bookmaker_odds,\n",
    "    'overrounds': overrounds,\n",
    "    'elo': elo_features,\n",
    "    'consensus': consensus_features,\n",
    "    'engineered': engineered_features\n",
    "}\n",
    "stability_df = group_stability_no_date_common_bins(X_train, groups, split_ratio=0.7, bins=10, verbose=True)\n",
    "print(stability_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3 style='font-size:20px;'>Cross Group Correlation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                g1              g2  mean_abs_corr\n",
      "1             form      overrounds       0.023592\n",
      "5   bookmaker_odds      overrounds       0.039066\n",
      "10      overrounds       consensus       0.039583\n",
      "11      overrounds      engineered       0.046445\n",
      "9       overrounds             elo       0.104737\n",
      "4             form      engineered       0.376694\n",
      "2             form             elo       0.387839\n",
      "0             form  bookmaker_odds       0.400544\n",
      "3             form       consensus       0.402658\n",
      "13             elo      engineered       0.575139\n"
     ]
    }
   ],
   "source": [
    "def cross_group_corr(X, groups):\n",
    "    # mean absolute correlation between every pair of groups\n",
    "    names = list(groups.keys())\n",
    "    data = []\n",
    "    for i in range(len(names)):\n",
    "        for j in range(i+1, len(names)):\n",
    "            gi = [f for f in groups[names[i]] if f in X.columns]\n",
    "            gj = [f for f in groups[names[j]] if f in X.columns]\n",
    "            if not gi or not gj:\n",
    "                continue\n",
    "            corr = X[gi+gj].corr().abs().loc[gi, gj].values\n",
    "            data.append({'g1':names[i], 'g2':names[j], 'mean_abs_corr': float(np.nanmean(corr))})\n",
    "    return pd.DataFrame(data).sort_values('mean_abs_corr')\n",
    "\n",
    "overlap_df = cross_group_corr(X_train, groups)\n",
    "print(overlap_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3 style='font-size:20px;'>Rank Group</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def rank_groups_multi(mi_df, redundancy_df, overlap_df, stability_df,\n",
    "                      mi_col=\"mi_top3_sum\",\n",
    "                      weights=None,          # e.g. {\"mi\":2, \"redundancy\":1, \"overlap\":1, \"stability\":1}\n",
    "                      sort_by=\"overall\"):    # \"overall\", \"mi\", \"redundancy\", \"overlap\", or \"stability\"\n",
    "    \"\"\"\n",
    "    Combine group diagnostics and produce ranks per criterion (and optional overall rank).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mi_df : DataFrame with columns [\"group\", mi_col] where mi_col is e.g. \"mi_top3_sum\"\n",
    "    redundancy_df : DataFrame with columns [\"group\", \"intragroup_corr_mean\"]\n",
    "    overlap_df : DataFrame with columns [\"g1\",\"g2\",\"mean_abs_corr\"] (pairwise group overlaps)\n",
    "    stability_df : DataFrame with columns [\"group\",\"psi_mean\"]\n",
    "    mi_col : str, column name in mi_df to use for MI ranking\n",
    "    weights : dict or None, weights for overall rank (keys: \"mi\",\"redundancy\",\"overlap\",\"stability\")\n",
    "    sort_by : str, which rank to sort by (\"overall\" requires weights)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame with raw metrics, ranks per criterion, and optional overall rank.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Cross-group overlap → per-group mean overlap (symmetrize pairs) ---\n",
    "    if overlap_df is not None and not overlap_df.empty:\n",
    "        og = pd.concat([\n",
    "            overlap_df.rename(columns={\"g1\": \"group\", \"g2\": \"other\"}),\n",
    "            overlap_df.rename(columns={\"g2\": \"group\", \"g1\": \"other\"})\n",
    "        ], ignore_index=True)\n",
    "        cross_mean = (og.groupby(\"group\")[\"mean_abs_corr\"]\n",
    "                        .mean()\n",
    "                        .rename(\"crossgroup_corr_mean\")\n",
    "                        .reset_index())\n",
    "    else:\n",
    "        cross_mean = pd.DataFrame(columns=[\"group\", \"crossgroup_corr_mean\"])\n",
    "\n",
    "    # --- Merge everything on 'group' ---\n",
    "    cols_mi = [\"group\", mi_col]\n",
    "    cols_rd = [\"group\", \"intragroup_corr_mean\"]\n",
    "    cols_st = [\"group\", \"psi_mean\"]\n",
    "\n",
    "    df = (mi_df[cols_mi]\n",
    "            .merge(redundancy_df[cols_rd], on=\"group\", how=\"outer\")\n",
    "            .merge(cross_mean, on=\"group\", how=\"outer\")\n",
    "            .merge(stability_df[cols_st], on=\"group\", how=\"outer\"))\n",
    "\n",
    "    # Helper to rank while pushing NaNs to the bottom (worst)\n",
    "    def _rank(series, ascending):\n",
    "        s = series.copy()\n",
    "        if s.isna().any():\n",
    "            fill = (s.max() + 1) if ascending else (s.min() - 1)\n",
    "            s = s.fillna(fill)\n",
    "        return s.rank(ascending=ascending, method=\"min\")\n",
    "\n",
    "    # --- Per-criterion ranks ---\n",
    "    df[\"rank_mi\"]          = _rank(df[mi_col], ascending=False)              # higher MI is better\n",
    "    df[\"rank_redundancy\"]  = _rank(df[\"intragroup_corr_mean\"], ascending=True)\n",
    "    df[\"rank_overlap\"]     = _rank(df[\"crossgroup_corr_mean\"], ascending=True)\n",
    "    df[\"rank_stability\"]   = _rank(df[\"psi_mean\"], ascending=True)\n",
    "\n",
    "    # --- Optional overall rank (weighted sum of ranks; lower = better) ---\n",
    "    if weights:\n",
    "        w_mi  = float(weights.get(\"mi\", 1.0))\n",
    "        w_rd  = float(weights.get(\"redundancy\", 1.0))\n",
    "        w_ov  = float(weights.get(\"overlap\", 1.0))\n",
    "        w_ps  = float(weights.get(\"stability\", 1.0))\n",
    "        df[\"overall_rank_score\"] = (\n",
    "            w_mi*df[\"rank_mi\"] +\n",
    "            w_rd*df[\"rank_redundancy\"] +\n",
    "            w_ov*df[\"rank_overlap\"] +\n",
    "            w_ps*df[\"rank_stability\"]\n",
    "        )\n",
    "        df[\"rank_overall\"] = df[\"overall_rank_score\"].rank(ascending=True, method=\"min\")\n",
    "\n",
    "    # --- Sorting ---\n",
    "    sort_map = {\n",
    "        \"overall\": \"rank_overall\",\n",
    "        \"mi\": \"rank_mi\",\n",
    "        \"redundancy\": \"rank_redundancy\",\n",
    "        \"overlap\": \"rank_overlap\",\n",
    "        \"stability\": \"rank_stability\",\n",
    "    }\n",
    "    sort_col = sort_map.get(sort_by, \"rank_mi\")\n",
    "    if sort_by == \"overall\" and not weights:\n",
    "        sort_col = \"rank_mi\"  # fallback if weights not provided\n",
    "\n",
    "    # Nice column order\n",
    "    out_cols = [\"group\", mi_col, \"intragroup_corr_mean\", \"crossgroup_corr_mean\", \"psi_mean\",\n",
    "                \"rank_mi\", \"rank_redundancy\", \"rank_overlap\", \"rank_stability\"]\n",
    "    if weights:\n",
    "        out_cols += [\"overall_rank_score\", \"rank_overall\"]\n",
    "\n",
    "    return df[out_cols].sort_values(sort_col, ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            group  mi_top3_sum  intragroup_corr_mean  crossgroup_corr_mean  \\\n",
      "0            form     0.084605              0.309717              0.318265   \n",
      "1  bookmaker_odds     0.346286              0.659672              0.480782   \n",
      "2      engineered     0.130372              0.330012              0.462187   \n",
      "3       consensus     0.241277              0.531874              0.482342   \n",
      "4             elo     0.179883              0.474944              0.463766   \n",
      "5      overrounds     0.063325              0.528862              0.050685   \n",
      "\n",
      "   psi_mean  rank_mi  rank_redundancy  rank_overlap  rank_stability  \\\n",
      "0  0.014941      5.0              1.0           2.0             1.0   \n",
      "1  0.075031      1.0              6.0           5.0             2.0   \n",
      "2  0.113055      4.0              2.0           3.0             4.0   \n",
      "3  0.084992      2.0              5.0           6.0             3.0   \n",
      "4  0.193702      3.0              3.0           4.0             5.0   \n",
      "5  4.316615      6.0              4.0           1.0             6.0   \n",
      "\n",
      "   overall_rank_score  rank_overall  \n",
      "0                14.0           1.0  \n",
      "1                15.0           2.0  \n",
      "2                17.0           3.0  \n",
      "3                18.0           4.0  \n",
      "4                18.0           4.0  \n",
      "5                23.0           6.0  \n"
     ]
    }
   ],
   "source": [
    "# Choose weights (tweak as you like)\n",
    "weights = {\"mi\": 2.0, \"redundancy\": 1.0, \"overlap\": 1.0, \"stability\": 1.0}\n",
    "\n",
    "ranked = rank_groups_multi(\n",
    "    mi_df=mi_df,\n",
    "    redundancy_df=redundancy_df,\n",
    "    overlap_df=overlap_df,\n",
    "    stability_df=stability_df,   # from your PSI-with-common-bins function\n",
    "    mi_col=\"mi_top3_sum\",\n",
    "    weights=weights,\n",
    "    sort_by=\"overall\"            # or \"mi\"/\"redundancy\"/\"overlap\"/\"stability\"\n",
    ")\n",
    "print(ranked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "feat_cats  = [\"HomeTeam\", \"AwayTeam\"]              \n",
    "feat_nums  = form_features\n",
    "feat_all   = feat_cats + feat_nums + bookmaker_odds\n",
    "\n",
    "Xtr = X_train[feat_all].copy()\n",
    "Xte = X_test[feat_all].copy()\n",
    "ytr, yte = y_train, y_test\n",
    "\n",
    "for c in feat_cats:\n",
    "    Xtr[c] = Xtr[c].astype(str)\n",
    "    Xte[c] = Xte[c].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from collections import Counter\n",
    "\n",
    "# Define feature groups\n",
    "feat_cats = [\"HomeTeam\", \"AwayTeam\"]              \n",
    "feat_nums = form_features\n",
    "feat_all = feat_cats + feat_nums + bookmaker_odds\n",
    "\n",
    "# Prepare data\n",
    "Xtr = X_train[feat_all].copy()\n",
    "Xte = X_test[feat_all].copy()\n",
    "ytr, yte = y_train, y_test\n",
    "\n",
    "# Ensure categorical features are strings\n",
    "for c in feat_cats:\n",
    "    Xtr[c] = Xtr[c].astype(str)\n",
    "    Xte[c] = Xte[c].astype(str)\n",
    "\n",
    "# --- ALWAYS encode categorical features first ---\n",
    "encoders = {}\n",
    "Xtr_enc = Xtr.copy()\n",
    "Xte_enc = Xte.copy()\n",
    "\n",
    "for c in feat_cats:\n",
    "    le = LabelEncoder()\n",
    "    # Fit on training data only, transform both train and test\n",
    "    Xtr_enc[c] = le.fit_transform(Xtr[c])\n",
    "    Xte_enc[c] = le.transform(Xte[c])  # Use transform, not fit_transform\n",
    "    encoders[c] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Counter({2: 2013, 0: 1178, 1: 989})\n",
      "After SMOTE: Counter({2: 2013, 0: 2013, 1: 2013})\n"
     ]
    }
   ],
   "source": [
    "# Get indices of categorical columns for SMOTENC\n",
    "cat_idx = [Xtr_enc.columns.get_loc(c) for c in feat_cats]\n",
    "\n",
    "# --- Apply SMOTENC ---\n",
    "smote = SMOTENC(\n",
    "    categorical_features=cat_idx,\n",
    "    sampling_strategy=\"not majority\",  # upsample draws & away, keep home\n",
    "    random_state=42\n",
    ")\n",
    "Xtr_bal, ytr_bal = smote.fit_resample(Xtr_enc, ytr)\n",
    "\n",
    "print(\"Before SMOTE:\", Counter(ytr))\n",
    "print(\"After SMOTE:\", Counter(ytr_bal))\n",
    "\n",
    "# --- For CatBoost: Keep encoded features ---\n",
    "# CatBoost works with encoded categorical features\n",
    "Xtr_final = pd.DataFrame(Xtr_bal, columns=Xtr_enc.columns)\n",
    "Xte_final = Xte_enc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, precision_recall_fscore_support\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "Xtr_bal, ytr_bal = shuffle(Xtr_bal, ytr_bal, random_state=42)\n",
    "\n",
    "def combos(grid):\n",
    "    keys = list(grid.keys())\n",
    "    for vals in itertools.product(*(grid[k] for k in keys)):\n",
    "        yield dict(zip(keys, vals))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:30px;'>Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': [0.05, 0.1, 0.15], 'depth': [6, 7, 8], 'l2_leaf_reg': [3, 5, 10], 'random_strength': [1.0, 2.0], 'bagging_temperature': [0.5, 1.0], 'rsm': [0.9, 1.0], 'grow_policy': ['SymmetricTree', 'Lossguide']}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>depth</th>\n",
       "      <th>l2_leaf_reg</th>\n",
       "      <th>random_strength</th>\n",
       "      <th>bagging_temperature</th>\n",
       "      <th>rsm</th>\n",
       "      <th>grow_policy</th>\n",
       "      <th>acc</th>\n",
       "      <th>logloss</th>\n",
       "      <th>c0_rec</th>\n",
       "      <th>c1_rec</th>\n",
       "      <th>c2_rec</th>\n",
       "      <th>c0_f1</th>\n",
       "      <th>c1_f1</th>\n",
       "      <th>c2_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.05</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.594444</td>\n",
       "      <td>0.983985</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.568421</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.701571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>0.978427</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.713568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0.10</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>0.993622</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.318841</td>\n",
       "      <td>0.700508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.05</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>0.995451</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.385542</td>\n",
       "      <td>0.698413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.05</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>0.996568</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.563107</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.701571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>1.002191</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0.15</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.977195</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.328358</td>\n",
       "      <td>0.701422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.981500</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.696970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.10</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.982129</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.696078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.05</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.983935</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.686869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.986519</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.328358</td>\n",
       "      <td>0.690355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.986603</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.328767</td>\n",
       "      <td>0.704663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.10</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.992988</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.289855</td>\n",
       "      <td>0.698413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.05</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.997163</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.05</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.997430</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.405063</td>\n",
       "      <td>0.670455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     learning_rate  depth  l2_leaf_reg  random_strength  bagging_temperature  \\\n",
       "143           0.05      8           10              2.0                  1.0   \n",
       "177           0.10      6           10              1.0                  0.5   \n",
       "283           0.10      8           10              2.0                  0.5   \n",
       "97            0.05      8            3              1.0                  0.5   \n",
       "141           0.05      8           10              2.0                  1.0   \n",
       "373           0.15      7           10              1.0                  1.0   \n",
       "415           0.15      8            5              2.0                  1.0   \n",
       "319           0.15      6            5              2.0                  1.0   \n",
       "281           0.10      8           10              2.0                  0.5   \n",
       "129           0.05      8           10              1.0                  0.5   \n",
       "41            0.05      6           10              2.0                  0.5   \n",
       "363           0.15      7            5              2.0                  0.5   \n",
       "263           0.10      8            5              1.0                  1.0   \n",
       "63            0.05      7            3              2.0                  1.0   \n",
       "117           0.05      8            5              1.0                  1.0   \n",
       "\n",
       "     rsm grow_policy       acc   logloss    c0_rec  c1_rec    c2_rec  \\\n",
       "143  1.0   Lossguide  0.594444  0.983985  0.574468    0.26  0.807229   \n",
       "177  0.9   Lossguide  0.588889  0.978427  0.489362    0.24  0.855422   \n",
       "283  1.0   Lossguide  0.588889  0.993622  0.553191    0.22  0.831325   \n",
       "97   0.9   Lossguide  0.588889  0.995451  0.510638    0.32  0.795181   \n",
       "141  0.9   Lossguide  0.588889  0.996568  0.617021    0.20  0.807229   \n",
       "373  0.9   Lossguide  0.588889  1.002191  0.574468    0.18  0.843373   \n",
       "415  1.0   Lossguide  0.583333  0.977195  0.425532    0.22  0.891566   \n",
       "319  1.0   Lossguide  0.583333  0.981500  0.574468    0.18  0.831325   \n",
       "281  0.9   Lossguide  0.583333  0.982129  0.489362    0.22  0.855422   \n",
       "129  0.9   Lossguide  0.583333  0.983935  0.553191    0.22  0.819277   \n",
       "41   0.9   Lossguide  0.583333  0.986519  0.553191    0.22  0.819277   \n",
       "363  1.0   Lossguide  0.583333  0.986603  0.531915    0.24  0.819277   \n",
       "263  1.0   Lossguide  0.583333  0.992988  0.617021    0.20  0.795181   \n",
       "63   1.0   Lossguide  0.583333  0.997163  0.574468    0.18  0.831325   \n",
       "117  0.9   Lossguide  0.583333  0.997430  0.638298    0.32  0.710843   \n",
       "\n",
       "        c0_f1     c1_f1     c2_f1  \n",
       "143  0.568421  0.351351  0.701571  \n",
       "177  0.541176  0.315789  0.713568  \n",
       "283  0.553191  0.318841  0.700508  \n",
       "97   0.545455  0.385542  0.698413  \n",
       "141  0.563107  0.303030  0.701571  \n",
       "373  0.580645  0.268657  0.700000  \n",
       "415  0.487805  0.328358  0.701422  \n",
       "319  0.551020  0.281250  0.696970  \n",
       "281  0.534884  0.314286  0.696078  \n",
       "129  0.541667  0.333333  0.686869  \n",
       "41   0.541667  0.328358  0.690355  \n",
       "363  0.531915  0.328767  0.704663  \n",
       "263  0.568627  0.289855  0.698413  \n",
       "63   0.574468  0.272727  0.690000  \n",
       "117  0.571429  0.405063  0.670455  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feat_cats + feat_nums + bookmaker_odds\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.05, 0.1, 0.15],\n",
    "    \"depth\": [6, 7, 8],\n",
    "    \"l2_leaf_reg\": [3, 5, 10],\n",
    "    \"random_strength\": [1.0, 2.0],\n",
    "    \"bagging_temperature\": [0.5, 1.0],\n",
    "    \"rsm\": [0.9, 1.0],\n",
    "    \"grow_policy\": [\"SymmetricTree\", \"Lossguide\"],  # compare both\n",
    "}\n",
    "\n",
    "print(param_grid)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for p in combos(param_grid):\n",
    "    model = CatBoostClassifier(\n",
    "        loss_function=\"MultiClass\",\n",
    "        eval_metric=\"Accuracy\",\n",
    "        iterations=1000,\n",
    "        verbose=False,\n",
    "        random_state=42,\n",
    "        **p\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        Xtr_bal, ytr_bal,\n",
    "        eval_set=(Xte, yte),\n",
    "        use_best_model=True,\n",
    "        cat_features=feat_cats        \n",
    "    )\n",
    "\n",
    "    y_pred  = model.predict(Xte)\n",
    "    y_proba = model.predict_proba(Xte)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(\n",
    "        yte, y_pred, zero_division=0\n",
    "    )\n",
    "\n",
    "    rows.append({\n",
    "        **p,\n",
    "        \"acc\": accuracy_score(yte, y_pred),\n",
    "        \"logloss\": log_loss(yte, y_proba),\n",
    "        \"c0_rec\": rc[0], \"c1_rec\": rc[1], \"c2_rec\": rc[2],\n",
    "        \"c0_f1\":  f1[0], \"c1_f1\":  f1[1], \"c2_f1\":  f1[2],\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(rows).sort_values([\"acc\",\"logloss\"], ascending=[False, True])\n",
    "display(results.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>depth</th>\n",
       "      <th>l2_leaf_reg</th>\n",
       "      <th>random_strength</th>\n",
       "      <th>bagging_temperature</th>\n",
       "      <th>rsm</th>\n",
       "      <th>grow_policy</th>\n",
       "      <th>max_leaves</th>\n",
       "      <th>acc</th>\n",
       "      <th>logloss</th>\n",
       "      <th>c0_rec</th>\n",
       "      <th>c1_rec</th>\n",
       "      <th>c2_rec</th>\n",
       "      <th>c0_f1</th>\n",
       "      <th>c1_f1</th>\n",
       "      <th>c2_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.20</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>32</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.983603</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.618182</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.738636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>32</td>\n",
       "      <td>0.594444</td>\n",
       "      <td>0.984467</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.695652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>32</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>0.980267</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.561798</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.696517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>32</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.990065</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.696078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>32</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.995174</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.276923</td>\n",
       "      <td>0.695652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>32</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.004221</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.328767</td>\n",
       "      <td>0.711864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>32</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.005975</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.563107</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.680851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>64</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.980976</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.238806</td>\n",
       "      <td>0.694301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>64</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.984662</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>0.690355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>32</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.984900</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.711340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.20</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>64</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>1.004135</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.516854</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.693467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.20</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>32</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>1.012891</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.563107</td>\n",
       "      <td>0.318841</td>\n",
       "      <td>0.680851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>64</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>1.017127</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.554455</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.663102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>32</td>\n",
       "      <td>0.572222</td>\n",
       "      <td>0.973424</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.309859</td>\n",
       "      <td>0.690722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>32</td>\n",
       "      <td>0.572222</td>\n",
       "      <td>0.973983</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.494382</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.689655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     learning_rate  depth  l2_leaf_reg  random_strength  bagging_temperature  \\\n",
       "252           0.20      7            2              1.0                  0.5   \n",
       "150           0.15      7            2              0.5                  0.5   \n",
       "28            0.10      6            3              1.0                  0.5   \n",
       "26            0.10      6            3              1.0                  0.3   \n",
       "54            0.10      7            2              0.5                  0.5   \n",
       "134           0.15      6           10              0.5                  0.5   \n",
       "170           0.15      7            3              1.0                  0.3   \n",
       "153           0.15      7            2              1.0                  0.3   \n",
       "135           0.15      6           10              0.5                  0.5   \n",
       "52            0.10      7            2              0.5                  0.5   \n",
       "235           0.20      6           10              1.0                  0.3   \n",
       "226           0.20      6           10              0.5                  0.3   \n",
       "127           0.15      6            3              1.0                  0.5   \n",
       "180           0.15      7           10              0.5                  0.5   \n",
       "186           0.15      7           10              1.0                  0.3   \n",
       "\n",
       "     rsm grow_policy  max_leaves       acc   logloss    c0_rec  c1_rec  \\\n",
       "252  0.9   Lossguide          32  0.611111  0.983603  0.723404    0.22   \n",
       "150  1.0   Lossguide          32  0.594444  0.984467  0.617021    0.28   \n",
       "28   0.9   Lossguide          32  0.588889  0.980267  0.531915    0.22   \n",
       "26   1.0   Lossguide          32  0.583333  0.990065  0.468085    0.24   \n",
       "54   1.0   Lossguide          32  0.583333  0.995174  0.510638    0.18   \n",
       "134  1.0   Lossguide          32  0.583333  1.004221  0.638298    0.24   \n",
       "170  1.0   Lossguide          32  0.583333  1.005975  0.617021    0.24   \n",
       "153  0.9   Lossguide          64  0.577778  0.980976  0.617021    0.16   \n",
       "135  1.0   Lossguide          64  0.577778  0.984662  0.489362    0.26   \n",
       "52   0.9   Lossguide          32  0.577778  0.984900  0.553191    0.18   \n",
       "235  1.0   Lossguide          64  0.577778  1.004135  0.489362    0.24   \n",
       "226  1.0   Lossguide          32  0.577778  1.012891  0.617021    0.22   \n",
       "127  1.0   Lossguide          64  0.577778  1.017127  0.595745    0.28   \n",
       "180  0.9   Lossguide          32  0.572222  0.973424  0.531915    0.22   \n",
       "186  1.0   Lossguide          32  0.572222  0.973983  0.468085    0.22   \n",
       "\n",
       "       c2_rec     c0_f1     c1_f1     c2_f1  \n",
       "252  0.783133  0.618182  0.297297  0.738636  \n",
       "150  0.771084  0.591837  0.358974  0.695652  \n",
       "28   0.843373  0.561798  0.314286  0.696517  \n",
       "26   0.855422  0.511628  0.342857  0.696078  \n",
       "54   0.867470  0.545455  0.276923  0.695652  \n",
       "134  0.759036  0.545455  0.328767  0.711864  \n",
       "170  0.771084  0.563107  0.347826  0.680851  \n",
       "153  0.807229  0.580000  0.238806  0.694301  \n",
       "135  0.819277  0.522727  0.346667  0.690355  \n",
       "52   0.831325  0.525253  0.268657  0.711340  \n",
       "235  0.831325  0.516854  0.333333  0.693467  \n",
       "226  0.771084  0.563107  0.318841  0.680851  \n",
       "127  0.746988  0.554455  0.388889  0.663102  \n",
       "180  0.807229  0.526316  0.309859  0.690722  \n",
       "186  0.843373  0.494382  0.323529  0.689655  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feat_cats + feat_nums + bookmaker_odds\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.1, 0.15, 0.2],\n",
    "    \"depth\": [6, 7],\n",
    "    \"l2_leaf_reg\": [2, 3, 10],\n",
    "    \"random_strength\": [0.5, 1.0],\n",
    "    \"bagging_temperature\": [0.3, 0.5],\n",
    "    \"rsm\": [0.9, 1.0],\n",
    "    \"grow_policy\": [\"Lossguide\"],\n",
    "    \"max_leaves\": [32, 64]\n",
    "}\n",
    "\n",
    "rows = []\n",
    "\n",
    "for p in combos(param_grid):\n",
    "    model = CatBoostClassifier(\n",
    "        loss_function=\"MultiClass\",\n",
    "        eval_metric=\"Accuracy\",\n",
    "        iterations=1000,\n",
    "        verbose=False,\n",
    "        random_state=42,\n",
    "        **p\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        Xtr_bal, ytr_bal,\n",
    "        eval_set=(Xte, yte),\n",
    "        use_best_model=True,\n",
    "        cat_features=feat_cats         \n",
    "    )\n",
    "\n",
    "    y_pred  = model.predict(Xte)\n",
    "    y_proba = model.predict_proba(Xte)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(\n",
    "        yte, y_pred, zero_division=0\n",
    "    )\n",
    "\n",
    "    rows.append({\n",
    "        **p,\n",
    "        \"acc\": accuracy_score(yte, y_pred),\n",
    "        \"logloss\": log_loss(yte, y_proba),\n",
    "        \"c0_rec\": rc[0], \"c1_rec\": rc[1], \"c2_rec\": rc[2],\n",
    "        \"c0_f1\":  f1[0], \"c1_f1\":  f1[1], \"c2_f1\":  f1[2],\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(rows).sort_values([\"acc\",\"logloss\"], ascending=[False, True])\n",
    "display(results.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>depth</th>\n",
       "      <th>l2_leaf_reg</th>\n",
       "      <th>random_strength</th>\n",
       "      <th>bagging_temperature</th>\n",
       "      <th>rsm</th>\n",
       "      <th>grow_policy</th>\n",
       "      <th>max_leaves</th>\n",
       "      <th>acc</th>\n",
       "      <th>logloss</th>\n",
       "      <th>c0_rec</th>\n",
       "      <th>c1_rec</th>\n",
       "      <th>c2_rec</th>\n",
       "      <th>c0_f1</th>\n",
       "      <th>c1_f1</th>\n",
       "      <th>c2_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0.16</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>32</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.959579</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.574257</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.708571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.90</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>20</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.982685</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.582524</td>\n",
       "      <td>0.376812</td>\n",
       "      <td>0.712766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0.16</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>16</td>\n",
       "      <td>0.605556</td>\n",
       "      <td>0.972337</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.695187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>0.18</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>20</td>\n",
       "      <td>0.605556</td>\n",
       "      <td>0.995310</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.577320</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.725389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0.17</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>32</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.965037</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>0.684783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>16</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.974729</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.539326</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>0.700508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.95</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>32</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.981227</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.366197</td>\n",
       "      <td>0.696970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.16</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>16</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.981863</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.711340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.16</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>16</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.990963</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.557692</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.715789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0.17</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>16</td>\n",
       "      <td>0.594444</td>\n",
       "      <td>0.974648</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.537634</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.710660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>0.18</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.90</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>24</td>\n",
       "      <td>0.594444</td>\n",
       "      <td>0.985635</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.410959</td>\n",
       "      <td>0.696517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0.17</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>16</td>\n",
       "      <td>0.594444</td>\n",
       "      <td>0.985780</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.394366</td>\n",
       "      <td>0.702703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>24</td>\n",
       "      <td>0.594444</td>\n",
       "      <td>0.994903</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.513761</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.741935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0.16</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.95</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>32</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>0.978780</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.366197</td>\n",
       "      <td>0.687831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.17</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.95</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>16</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>0.979242</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.638554</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.654321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     learning_rate  depth  l2_leaf_reg  random_strength  bagging_temperature  \\\n",
       "223           0.16      7            1              1.0                 0.20   \n",
       "13            0.15      6            1              1.0                 0.25   \n",
       "284           0.16      7            2              1.0                 0.30   \n",
       "537           0.18      7            1              1.0                 0.30   \n",
       "407           0.17      7            2              1.0                 0.20   \n",
       "32            0.15      6            1              1.0                 0.30   \n",
       "67            0.15      6            2              1.0                 0.30   \n",
       "148           0.16      6            1              1.0                 0.20   \n",
       "256           0.16      7            2              1.0                 0.20   \n",
       "332           0.17      6            2              1.0                 0.20   \n",
       "458           0.18      6            1              1.0                 0.30   \n",
       "308           0.17      6            1              1.0                 0.25   \n",
       "118           0.15      7            2              1.0                 0.20   \n",
       "283           0.16      7            2              1.0                 0.30   \n",
       "364           0.17      7            1              1.0                 0.20   \n",
       "\n",
       "      rsm grow_policy  max_leaves       acc   logloss    c0_rec  c1_rec  \\\n",
       "223  0.95   Lossguide          32  0.611111  0.959579  0.617021    0.38   \n",
       "13   0.90   Lossguide          20  0.611111  0.982685  0.638298    0.26   \n",
       "284  1.00   Lossguide          16  0.605556  0.972337  0.680851    0.24   \n",
       "537  1.00   Lossguide          20  0.605556  0.995310  0.595745    0.22   \n",
       "407  1.00   Lossguide          32  0.600000  0.965037  0.638298    0.30   \n",
       "32   1.00   Lossguide          16  0.600000  0.974729  0.510638    0.30   \n",
       "67   0.95   Lossguide          32  0.600000  0.981227  0.553191    0.26   \n",
       "148  0.95   Lossguide          16  0.600000  0.981863  0.595745    0.22   \n",
       "256  0.95   Lossguide          16  0.600000  0.990963  0.617021    0.22   \n",
       "332  1.00   Lossguide          16  0.594444  0.974648  0.531915    0.24   \n",
       "458  0.90   Lossguide          24  0.594444  0.985635  0.468085    0.30   \n",
       "308  1.00   Lossguide          16  0.594444  0.985780  0.595745    0.28   \n",
       "118  1.00   Lossguide          24  0.594444  0.994903  0.595745    0.20   \n",
       "283  0.95   Lossguide          32  0.588889  0.978780  0.595745    0.26   \n",
       "364  0.95   Lossguide          16  0.588889  0.979242  0.702128    0.40   \n",
       "\n",
       "       c2_rec     c0_f1     c1_f1     c2_f1  \n",
       "223  0.746988  0.574257  0.452381  0.708571  \n",
       "13   0.807229  0.582524  0.376812  0.712766  \n",
       "284  0.783133  0.621359  0.342857  0.695187  \n",
       "537  0.843373  0.577320  0.314286  0.725389  \n",
       "407  0.759036  0.588235  0.405405  0.684783  \n",
       "32   0.831325  0.539326  0.405405  0.700508  \n",
       "67   0.831325  0.571429  0.366197  0.696970  \n",
       "148  0.831325  0.560000  0.333333  0.711340  \n",
       "256  0.819277  0.557692  0.333333  0.715789  \n",
       "332  0.843373  0.537634  0.342857  0.710660  \n",
       "458  0.843373  0.511628  0.410959  0.696517  \n",
       "308  0.783133  0.538462  0.394366  0.702703  \n",
       "118  0.831325  0.513761  0.307692  0.741935  \n",
       "283  0.783133  0.560000  0.366197  0.687831  \n",
       "364  0.638554  0.611111  0.444444  0.654321  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feat_cats + feat_nums + bookmaker_odds\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.15, 0.16, 0.17, 0.18],\n",
    "    \"depth\": [6, 7],\n",
    "    \"l2_leaf_reg\": [1, 2],\n",
    "    \"random_strength\": [1.0],\n",
    "    \"bagging_temperature\": [0.2, 0.25, 0.3],\n",
    "    \"rsm\": [0.9, 0.95, 1.0],\n",
    "    \"grow_policy\": [\"Lossguide\"],\n",
    "    \"max_leaves\": [16, 20, 24, 32]\n",
    "}\n",
    "\n",
    "rows = []\n",
    "\n",
    "for p in combos(param_grid):\n",
    "    model = CatBoostClassifier(\n",
    "        loss_function=\"MultiClass\",\n",
    "        eval_metric=\"Accuracy\",\n",
    "        iterations=1000,\n",
    "        verbose=False,\n",
    "        random_state=42,\n",
    "        **p\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        Xtr_bal, ytr_bal,\n",
    "        eval_set=(Xte, yte),\n",
    "        use_best_model=True,\n",
    "        cat_features=feat_cats\n",
    "    )\n",
    "\n",
    "    y_pred  = model.predict(Xte)\n",
    "    y_proba = model.predict_proba(Xte)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(\n",
    "        yte, y_pred, zero_division=0\n",
    "    )\n",
    "\n",
    "    rows.append({\n",
    "        **p,\n",
    "        \"acc\": accuracy_score(yte, y_pred),\n",
    "        \"logloss\": log_loss(yte, y_proba),\n",
    "        \"c0_rec\": rc[0], \"c1_rec\": rc[1], \"c2_rec\": rc[2],\n",
    "        \"c0_f1\":  f1[0], \"c1_f1\":  f1[1], \"c2_f1\":  f1[2],\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(rows).sort_values([\"acc\",\"logloss\"], ascending=[False, True])\n",
    "display(results.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grow_policy</th>\n",
       "      <th>depth</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>l2_leaf_reg</th>\n",
       "      <th>min_data_in_leaf</th>\n",
       "      <th>max_leaves</th>\n",
       "      <th>rsm</th>\n",
       "      <th>bagging_temperature</th>\n",
       "      <th>random_strength</th>\n",
       "      <th>acc</th>\n",
       "      <th>logloss</th>\n",
       "      <th>c0_rec</th>\n",
       "      <th>c1_rec</th>\n",
       "      <th>c2_rec</th>\n",
       "      <th>c0_f1</th>\n",
       "      <th>c1_f1</th>\n",
       "      <th>c2_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>Lossguide</td>\n",
       "      <td>7</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.5</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.971068</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.723164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>Lossguide</td>\n",
       "      <td>6</td>\n",
       "      <td>0.18</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.962962</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.594059</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.715789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>Lossguide</td>\n",
       "      <td>7</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.5</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.976561</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.388060</td>\n",
       "      <td>0.707692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>Lossguide</td>\n",
       "      <td>7</td>\n",
       "      <td>0.17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.981151</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.771084</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.699454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>Lossguide</td>\n",
       "      <td>7</td>\n",
       "      <td>0.16</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.989073</td>\n",
       "      <td>0.659574</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.558559</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.734940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>Lossguide</td>\n",
       "      <td>6</td>\n",
       "      <td>0.18</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.605556</td>\n",
       "      <td>0.973894</td>\n",
       "      <td>0.659574</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.379747</td>\n",
       "      <td>0.696133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2482</th>\n",
       "      <td>Lossguide</td>\n",
       "      <td>7</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.605556</td>\n",
       "      <td>0.978975</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.612613</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.682081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2675</th>\n",
       "      <td>Lossguide</td>\n",
       "      <td>7</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.605556</td>\n",
       "      <td>0.983217</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>0.703518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>Lossguide</td>\n",
       "      <td>7</td>\n",
       "      <td>0.17</td>\n",
       "      <td>2.5</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.605556</td>\n",
       "      <td>0.990319</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.701031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>Lossguide</td>\n",
       "      <td>7</td>\n",
       "      <td>0.16</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.605556</td>\n",
       "      <td>1.002430</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.621359</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.704663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>Lossguide</td>\n",
       "      <td>7</td>\n",
       "      <td>0.17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.964217</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.707071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lossguide</td>\n",
       "      <td>6</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.980169</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.378378</td>\n",
       "      <td>0.707071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Lossguide</td>\n",
       "      <td>6</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.981834</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.702128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Lossguide</td>\n",
       "      <td>6</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.982096</td>\n",
       "      <td>0.659574</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.596154</td>\n",
       "      <td>0.409639</td>\n",
       "      <td>0.693642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>Lossguide</td>\n",
       "      <td>7</td>\n",
       "      <td>0.17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.982927</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.537634</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.722513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     grow_policy  depth  learning_rate  l2_leaf_reg  min_data_in_leaf  \\\n",
       "1729   Lossguide      7           0.16          2.5                12   \n",
       "1297   Lossguide      6           0.18          3.0                 5   \n",
       "1748   Lossguide      7           0.16          2.5                12   \n",
       "2386   Lossguide      7           0.17          3.0                12   \n",
       "1900   Lossguide      7           0.16          3.0                12   \n",
       "1356   Lossguide      6           0.18          3.0                 8   \n",
       "2482   Lossguide      7           0.18          2.0                 5   \n",
       "2675   Lossguide      7           0.18          2.5                 8   \n",
       "2238   Lossguide      7           0.17          2.5                12   \n",
       "1920   Lossguide      7           0.16          3.0                12   \n",
       "2370   Lossguide      7           0.17          3.0                 8   \n",
       "0      Lossguide      6           0.16          2.0                 5   \n",
       "62     Lossguide      6           0.16          2.0                 8   \n",
       "106    Lossguide      6           0.16          2.0                 8   \n",
       "2298   Lossguide      7           0.17          3.0                 5   \n",
       "\n",
       "      max_leaves   rsm  bagging_temperature  random_strength       acc  \\\n",
       "1729          20  0.85                 0.20              1.0  0.622222   \n",
       "1297          20  0.85                 0.20              1.0  0.611111   \n",
       "1748          24  0.85                 0.25              0.5  0.611111   \n",
       "2386          20  0.87                 0.30              0.5  0.611111   \n",
       "1900          20  0.87                 0.30              0.5  0.611111   \n",
       "1356          20  0.87                 0.20              0.5  0.605556   \n",
       "2482          28  0.90                 0.30              0.5  0.605556   \n",
       "2675          24  0.87                 0.30              1.0  0.605556   \n",
       "2238          24  0.87                 0.20              0.5  0.605556   \n",
       "1920          24  0.90                 0.20              0.5  0.605556   \n",
       "2370          28  0.90                 0.20              0.5  0.600000   \n",
       "0             20  0.85                 0.20              0.5  0.600000   \n",
       "62            20  0.87                 0.25              0.5  0.600000   \n",
       "106           28  0.90                 0.30              0.5  0.600000   \n",
       "2298          24  0.90                 0.20              0.5  0.600000   \n",
       "\n",
       "       logloss    c0_rec  c1_rec    c2_rec     c0_f1     c1_f1     c2_f1  \n",
       "1729  0.971068  0.680851    0.32  0.771084  0.609524  0.410256  0.723164  \n",
       "1297  0.962962  0.638298    0.24  0.819277  0.594059  0.347826  0.715789  \n",
       "1748  0.976561  0.595745    0.26  0.831325  0.571429  0.388060  0.707692  \n",
       "2386  0.981151  0.595745    0.36  0.771084  0.583333  0.444444  0.699454  \n",
       "1900  0.989073  0.659574    0.36  0.734940  0.558559  0.433735  0.734940  \n",
       "1356  0.973894  0.659574    0.30  0.759036  0.620000  0.379747  0.696133  \n",
       "2482  0.978975  0.723404    0.32  0.710843  0.612613  0.421053  0.682081  \n",
       "2675  0.983217  0.510638    0.30  0.843373  0.551724  0.405405  0.703518  \n",
       "2238  0.990319  0.574468    0.28  0.819277  0.562500  0.400000  0.701031  \n",
       "1920  1.002430  0.680851    0.18  0.819277  0.621359  0.281250  0.704663  \n",
       "2370  0.964217  0.510638    0.28  0.843373  0.521739  0.400000  0.707071  \n",
       "0     0.980169  0.510638    0.28  0.843373  0.545455  0.378378  0.707071  \n",
       "62    0.981834  0.574468    0.30  0.795181  0.574468  0.384615  0.702128  \n",
       "106   0.982096  0.659574    0.34  0.722892  0.596154  0.409639  0.693642  \n",
       "2298  0.982927  0.531915    0.28  0.831325  0.537634  0.368421  0.722513  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feat_cats + feat_nums + bookmaker_odds\n",
    "\n",
    "param_grid = {\n",
    "    \"grow_policy\": [\"Lossguide\"],\n",
    "    \"depth\": [6, 7],\n",
    "    \"learning_rate\": [0.16, 0.17, 0.18],            \n",
    "    \"l2_leaf_reg\": [2.0, 2.5, 3],\n",
    "    \"min_data_in_leaf\": [5, 8, 12],\n",
    "    \"max_leaves\": [20, 24, 28],\n",
    "    \"rsm\": [0.85, 0.87, 0.90],\n",
    "    \"bagging_temperature\": [0.20, 0.25, 0.3],\n",
    "    \"random_strength\": [0.5, 1.0],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "for p in combos(param_grid):\n",
    "    model = CatBoostClassifier(\n",
    "        loss_function=\"MultiClass\",\n",
    "        eval_metric=\"Accuracy\",\n",
    "        iterations=1000,\n",
    "        verbose=False,\n",
    "        random_state=42,\n",
    "        **p\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        Xtr_bal, ytr_bal,\n",
    "        eval_set=(Xte, yte),\n",
    "        use_best_model=True,\n",
    "        cat_features=feat_cats\n",
    "    )\n",
    "\n",
    "    y_pred  = model.predict(Xte)\n",
    "    y_proba = model.predict_proba(Xte)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(\n",
    "        yte, y_pred, zero_division=0\n",
    "    )\n",
    "\n",
    "    rows.append({\n",
    "        **p,\n",
    "        \"acc\": accuracy_score(yte, y_pred),\n",
    "        \"logloss\": log_loss(yte, y_proba),\n",
    "        \"c0_rec\": rc[0], \"c1_rec\": rc[1], \"c2_rec\": rc[2],\n",
    "        \"c0_f1\":  f1[0], \"c1_f1\":  f1[1], \"c2_f1\":  f1[2],\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(rows).sort_values([\"acc\",\"logloss\"], ascending=[False, True])\n",
    "display(results.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.62\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.68      0.61        47\n",
      "           1       0.57      0.32      0.41        50\n",
      "           2       0.68      0.77      0.72        83\n",
      "\n",
      "    accuracy                           0.62       180\n",
      "   macro avg       0.60      0.59      0.58       180\n",
      "weighted avg       0.62      0.62      0.61       180\n",
      "\n",
      "{'learn': {'Accuracy': 0.98642159297897, 'MultiClass': 0.24309875922298613}, 'validation': {'Accuracy': 0.6222222222222222, 'MultiClass': 0.9697007690746107}}\n"
     ]
    }
   ],
   "source": [
    "# feat_cats + feat_nums + bookmaker_odds\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "model = CatBoostClassifier(iterations=1000, loss_function=\"MultiClass\", grow_policy='Lossguide', random_strength=1.0, bagging_temperature=0.20, rsm=0.85,\n",
    "                           eval_metric=\"Accuracy\", learning_rate=0.16, random_state=42, depth=7,\n",
    "                           l2_leaf_reg=2.5, min_data_in_leaf=12, max_leaves=20)\n",
    "\n",
    "model.fit(Xtr_bal,\n",
    "          ytr_bal,\n",
    "          eval_set=(Xte, yte),\n",
    "          cat_features=feat_cats,    \n",
    "          verbose=False)\n",
    "\n",
    "y_pred = model.predict(Xte)\n",
    "print(f\"Test Set Accuracy: {accuracy_score(yte, y_pred):.2f}\")\n",
    "print(classification_report(yte, y_pred, zero_division=0))\n",
    "print(model.get_best_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, precision_score\n",
    "\n",
    "def run_model(classifier, param_grid, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, scoring='balanced_accuracy',\n",
    "                               cv=5, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    model = grid_search.best_estimator_\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:30px;'>Random Forest Classifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Test Set Accuracy: 0.55\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.43      0.47        47\n",
      "           1       0.48      0.26      0.34        50\n",
      "           2       0.57      0.80      0.67        83\n",
      "\n",
      "    accuracy                           0.55       180\n",
      "   macro avg       0.53      0.49      0.49       180\n",
      "weighted avg       0.54      0.55      0.52       180\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    }\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "best_params_rf = run_model(classifier, param_grid, Xtr_enc, ytr, Xte_enc, yte)\n",
    "best_params_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.55\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.43      0.47        47\n",
      "           1       0.48      0.26      0.34        50\n",
      "           2       0.57      0.80      0.67        83\n",
      "\n",
      "    accuracy                           0.55       180\n",
      "   macro avg       0.53      0.49      0.49       180\n",
      "weighted avg       0.54      0.55      0.52       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(random_state=42, n_estimators=100, min_samples_leaf=4, min_samples_split=10)\n",
    "classifier.fit(Xtr_enc, ytr)\n",
    "y_pred = classifier.predict(Xte_enc)\n",
    "print(f\"Test Set Accuracy: {accuracy_score(yte, y_pred):.2f}\")\n",
    "print(classification_report(yte, y_pred, zero_division=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:30px;'>Gradient Boosting Classifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.51\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.43      0.44        47\n",
      "           1       0.39      0.22      0.28        50\n",
      "           2       0.55      0.72      0.62        83\n",
      "\n",
      "    accuracy                           0.51       180\n",
      "   macro avg       0.47      0.46      0.45       180\n",
      "weighted avg       0.48      0.51      0.48       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=1000, max_depth=10)\n",
    "model.fit(Xtr_enc, ytr)\n",
    "y_pred = model.predict(Xte_enc)\n",
    "print(f\"Test Set Accuracy: {accuracy_score(yte, y_pred):.2f}\")\n",
    "print(classification_report(yte, y_pred, zero_division=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:30px;'>Naive Bayes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Test Set Accuracy: 0.49\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.55      0.50        47\n",
      "           1       0.34      0.46      0.39        50\n",
      "           2       0.70      0.47      0.56        83\n",
      "\n",
      "    accuracy                           0.49       180\n",
      "   macro avg       0.50      0.49      0.48       180\n",
      "weighted avg       0.54      0.49      0.50       180\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'priors': [0.3, 0.4, 0.3], 'var_smoothing': 2.1544346900318868e-11}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "param_grid = {\n",
    "    'var_smoothing': np.logspace(0, -12, num=10),\n",
    "    'priors': [[0.3, 0.4, 0.3]]\n",
    "}\n",
    "classifier = GaussianNB()\n",
    "\n",
    "best_params_nb = run_model(classifier, param_grid, Xtr_enc, ytr, Xte_enc, yte)\n",
    "best_params_nb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:30px;'>Stacking Classifier</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:15px;'>By Using Stacking Classifier, we can have a more balanced result which have a better performance in predicting results for Draw</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.53\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.62      0.53        47\n",
      "           1       0.42      0.30      0.35        50\n",
      "           2       0.64      0.63      0.63        83\n",
      "\n",
      "    accuracy                           0.53       180\n",
      "   macro avg       0.51      0.51      0.50       180\n",
      "weighted avg       0.53      0.53      0.53       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "\n",
    "classifier_1 = GradientBoostingClassifier(n_estimators=1000, criterion='friedman_mse', learning_rate=0.1, subsample=0.5)\n",
    "classifier_2 = RandomForestClassifier(n_estimators=1000, min_samples_leaf=1, max_leaf_nodes=5)\n",
    "classifier_3 = GaussianNB(var_smoothing=1e-09)\n",
    "sclf = StackingClassifier(estimators = [('rf', classifier_2), ('gb', classifier_1), ('gnb', classifier_3)],\n",
    "                          final_estimator = classifier_3\n",
    "                          )\n",
    "\n",
    "model = sclf.fit(Xtr_enc, ytr)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Test Set Accuracy: {accuracy_score(yte, y_pred):.2f}\")\n",
    "print(classification_report(yte, y_pred, zero_division=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:30px;'>CatBoost</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:30px;'>XGBoost</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Test Set Accuracy: 0.52\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.43      0.45        47\n",
      "           1       0.37      0.20      0.26        50\n",
      "           2       0.57      0.76      0.65        83\n",
      "\n",
      "    accuracy                           0.52       180\n",
      "   macro avg       0.47      0.46      0.45       180\n",
      "weighted avg       0.49      0.52      0.49       180\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 5,\n",
       " 'n_estimators': 1000,\n",
       " 'objective': 'multi:softmax'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "grid_params = {\n",
    "    'max_depth': [3,6,9],\n",
    "    'min_child_weight': [1,3,5],\n",
    "    'learning_rate': [0.1, 0.5, 1],\n",
    "    'objective': ['multi:softmax'],\n",
    "    'n_estimators': [1000]\n",
    "}\n",
    "\n",
    "classifier = XGBClassifier()\n",
    "best_params_gb = run_model(classifier, grid_params, X_train, y_train, X_test, y_test)\n",
    "best_params_gb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:30px;'>Save Model</h1>\n",
    "\n",
    "# <h1 style='font-size:15px;'>Catboost is the most efficient model such that it has best balanced prediction result in all 3 possible outcomes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "catboost = CatBoostClassifier(iterations=1000, loss_function=\"MultiClass\", \n",
    "                                eval_metric=\"Accuracy\", learning_rate=0.3, l2_leaf_reg=9, class_weights=[1, 1.5, 1])\n",
    "\n",
    "catboost.fit(X_train,\n",
    "          y_train,\n",
    "          eval_set=(X_test, y_test),\n",
    "          verbose=False)\n",
    "\n",
    "pickle.dump(catboost, open('catboost.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
