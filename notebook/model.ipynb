{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep full versions with context columns\n",
    "X_train_full = pd.read_csv(\"../data/X_train.csv\", index_col=0)\n",
    "X_test_full  = pd.read_csv(\"../data/X_test.csv\", index_col=0)\n",
    "\n",
    "y_train = pd.read_csv(\"../data/y_train.csv\", index_col=0).squeeze()\n",
    "y_test  = pd.read_csv(\"../data/y_test.csv\", index_col=0).squeeze()\n",
    "\n",
    "# Make stripped-down versions for the model\n",
    "non_features = [\"Div\", \"Date\", \n",
    "                \"HomeTeam_ShotOnTarget\", \"AwayTeam_ShotOnTarget\"]\n",
    "\n",
    "X_train_features = X_train_full.drop(columns=non_features)\n",
    "X_test_features  = X_test_full.drop(columns=non_features)\n",
    "\n",
    "# Save the feature order from the stripped-down version\n",
    "feature_columns = X_train_features.columns\n",
    "feature_columns.to_series(name=\"feature\").to_csv(\"../data/feature_columns.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.066188\n",
       "1       1.056489\n",
       "2       1.066188\n",
       "3       1.065873\n",
       "4       1.070850\n",
       "          ...   \n",
       "4175    1.046154\n",
       "4176    1.065605\n",
       "4177    1.050770\n",
       "4178    1.054094\n",
       "4179    1.057995\n",
       "Name: B365_overround, Length: 4180, dtype: float64"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_features['B365_overround']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feat_order = pd.read_csv(\"../data/feature_columns.csv\")[\"feature\"].tolist()\n",
    "\n",
    "def align_features(df: pd.DataFrame, feat_order: list[str]) -> pd.DataFrame:\n",
    "    # add any missing training columns as zeros\n",
    "    missing = [c for c in feat_order if c not in df.columns]\n",
    "    if missing:\n",
    "        df = df.copy()\n",
    "        for c in missing:\n",
    "            df[c] = 0.0\n",
    "\n",
    "    # drop any extras not used in training\n",
    "    extra = [c for c in df.columns if c not in feat_order]\n",
    "    if extra:\n",
    "        df = df.drop(columns=extra, errors=\"ignore\")\n",
    "\n",
    "    # put in the exact training order\n",
    "    df = df.reindex(columns=feat_order, fill_value=0.0)\n",
    "    return df\n",
    "\n",
    "# build the actual matrices the model will see\n",
    "X_train = align_features(X_train_features, feat_order)\n",
    "X_test  = align_features(X_test_features,  feat_order)\n",
    "\n",
    "# optional safety check\n",
    "assert list(X_train.columns) == feat_order == list(X_test.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomeTeam</th>\n",
       "      <th>AwayTeam</th>\n",
       "      <th>HomeTeam_avg_goal_diff</th>\n",
       "      <th>HomeTeam_points</th>\n",
       "      <th>AwayTeam_avg_goal_diff</th>\n",
       "      <th>AwayTeam_points</th>\n",
       "      <th>B365H</th>\n",
       "      <th>B365D</th>\n",
       "      <th>B365A</th>\n",
       "      <th>BWH</th>\n",
       "      <th>...</th>\n",
       "      <th>PS_overround</th>\n",
       "      <th>PSC_overround</th>\n",
       "      <th>pH_mean</th>\n",
       "      <th>pD_mean</th>\n",
       "      <th>pA_mean</th>\n",
       "      <th>overround_mean</th>\n",
       "      <th>overround_std</th>\n",
       "      <th>home_adv</th>\n",
       "      <th>draw_tightness</th>\n",
       "      <th>elo_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Espanol</td>\n",
       "      <td>Valladolid</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.468960</td>\n",
       "      <td>0.284218</td>\n",
       "      <td>0.246821</td>\n",
       "      <td>0.501789</td>\n",
       "      <td>...</td>\n",
       "      <td>1.084635</td>\n",
       "      <td>1.084635</td>\n",
       "      <td>0.486191</td>\n",
       "      <td>0.282183</td>\n",
       "      <td>0.231626</td>\n",
       "      <td>1.087405</td>\n",
       "      <td>0.042728</td>\n",
       "      <td>0.254565</td>\n",
       "      <td>0.282183</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Valencia</td>\n",
       "      <td>Mallorca</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.556783</td>\n",
       "      <td>0.262925</td>\n",
       "      <td>0.180292</td>\n",
       "      <td>0.548686</td>\n",
       "      <td>...</td>\n",
       "      <td>1.086819</td>\n",
       "      <td>1.086819</td>\n",
       "      <td>0.544034</td>\n",
       "      <td>0.266139</td>\n",
       "      <td>0.189827</td>\n",
       "      <td>1.088623</td>\n",
       "      <td>0.037464</td>\n",
       "      <td>0.354208</td>\n",
       "      <td>0.266139</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ath Bilbao</td>\n",
       "      <td>Almeria</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.468960</td>\n",
       "      <td>0.284218</td>\n",
       "      <td>0.246821</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>...</td>\n",
       "      <td>1.090072</td>\n",
       "      <td>1.090072</td>\n",
       "      <td>0.467183</td>\n",
       "      <td>0.285318</td>\n",
       "      <td>0.247500</td>\n",
       "      <td>1.089762</td>\n",
       "      <td>0.033230</td>\n",
       "      <td>0.219683</td>\n",
       "      <td>0.285318</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ath Madrid</td>\n",
       "      <td>Malaga</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.651526</td>\n",
       "      <td>0.223380</td>\n",
       "      <td>0.125093</td>\n",
       "      <td>0.647450</td>\n",
       "      <td>...</td>\n",
       "      <td>1.084317</td>\n",
       "      <td>1.084317</td>\n",
       "      <td>0.642133</td>\n",
       "      <td>0.234665</td>\n",
       "      <td>0.123202</td>\n",
       "      <td>1.082695</td>\n",
       "      <td>0.041275</td>\n",
       "      <td>0.518931</td>\n",
       "      <td>0.234665</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Betis</td>\n",
       "      <td>Recreativo</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.466919</td>\n",
       "      <td>0.287335</td>\n",
       "      <td>0.245747</td>\n",
       "      <td>0.518664</td>\n",
       "      <td>...</td>\n",
       "      <td>1.085347</td>\n",
       "      <td>1.085347</td>\n",
       "      <td>0.492309</td>\n",
       "      <td>0.283573</td>\n",
       "      <td>0.224118</td>\n",
       "      <td>1.085243</td>\n",
       "      <td>0.041920</td>\n",
       "      <td>0.268191</td>\n",
       "      <td>0.283573</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>Levante</td>\n",
       "      <td>Ath Madrid</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.271493</td>\n",
       "      <td>...</td>\n",
       "      <td>1.025927</td>\n",
       "      <td>1.026803</td>\n",
       "      <td>0.260182</td>\n",
       "      <td>0.250866</td>\n",
       "      <td>0.488952</td>\n",
       "      <td>1.037138</td>\n",
       "      <td>0.016111</td>\n",
       "      <td>-0.228769</td>\n",
       "      <td>0.250866</td>\n",
       "      <td>-235.644217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>Sevilla</td>\n",
       "      <td>Ath Bilbao</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.507262</td>\n",
       "      <td>0.276010</td>\n",
       "      <td>0.216728</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>...</td>\n",
       "      <td>1.026614</td>\n",
       "      <td>1.026469</td>\n",
       "      <td>0.501490</td>\n",
       "      <td>0.285331</td>\n",
       "      <td>0.213179</td>\n",
       "      <td>1.040258</td>\n",
       "      <td>0.019152</td>\n",
       "      <td>0.288311</td>\n",
       "      <td>0.285331</td>\n",
       "      <td>7.004375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177</th>\n",
       "      <td>Valladolid</td>\n",
       "      <td>Valencia</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.118960</td>\n",
       "      <td>0.181273</td>\n",
       "      <td>0.699767</td>\n",
       "      <td>0.119069</td>\n",
       "      <td>...</td>\n",
       "      <td>1.032115</td>\n",
       "      <td>1.034061</td>\n",
       "      <td>0.119226</td>\n",
       "      <td>0.192054</td>\n",
       "      <td>0.688720</td>\n",
       "      <td>1.039219</td>\n",
       "      <td>0.014740</td>\n",
       "      <td>-0.569493</td>\n",
       "      <td>0.192054</td>\n",
       "      <td>-161.814985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4178</th>\n",
       "      <td>Eibar</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.263523</td>\n",
       "      <td>0.237171</td>\n",
       "      <td>0.499307</td>\n",
       "      <td>0.257662</td>\n",
       "      <td>...</td>\n",
       "      <td>1.028292</td>\n",
       "      <td>1.027454</td>\n",
       "      <td>0.249476</td>\n",
       "      <td>0.232252</td>\n",
       "      <td>0.518272</td>\n",
       "      <td>1.040577</td>\n",
       "      <td>0.015316</td>\n",
       "      <td>-0.268796</td>\n",
       "      <td>0.232252</td>\n",
       "      <td>-302.934468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4179</th>\n",
       "      <td>Real Madrid</td>\n",
       "      <td>Betis</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.317073</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.602028</td>\n",
       "      <td>0.198986</td>\n",
       "      <td>0.198986</td>\n",
       "      <td>0.608078</td>\n",
       "      <td>...</td>\n",
       "      <td>1.031156</td>\n",
       "      <td>1.032229</td>\n",
       "      <td>0.622459</td>\n",
       "      <td>0.193352</td>\n",
       "      <td>0.184189</td>\n",
       "      <td>1.038654</td>\n",
       "      <td>0.017043</td>\n",
       "      <td>0.438270</td>\n",
       "      <td>0.193352</td>\n",
       "      <td>190.665160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4180 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         HomeTeam    AwayTeam  HomeTeam_avg_goal_diff  HomeTeam_points  \\\n",
       "0         Espanol  Valladolid                   0.375         0.000000   \n",
       "1        Valencia    Mallorca                   0.375         0.000000   \n",
       "2      Ath Bilbao     Almeria                   0.375         0.000000   \n",
       "3      Ath Madrid      Malaga                   0.375         0.000000   \n",
       "4           Betis  Recreativo                   0.375         0.000000   \n",
       "...           ...         ...                     ...              ...   \n",
       "4175      Levante  Ath Madrid                   0.550         0.666667   \n",
       "4176      Sevilla  Ath Bilbao                   0.325         0.266667   \n",
       "4177   Valladolid    Valencia                   0.425         0.666667   \n",
       "4178        Eibar   Barcelona                   0.325         0.400000   \n",
       "4179  Real Madrid       Betis                   0.400         0.466667   \n",
       "\n",
       "      AwayTeam_avg_goal_diff  AwayTeam_points     B365H     B365D     B365A  \\\n",
       "0                   0.439024         0.000000  0.468960  0.284218  0.246821   \n",
       "1                   0.439024         0.000000  0.556783  0.262925  0.180292   \n",
       "2                   0.439024         0.000000  0.468960  0.284218  0.246821   \n",
       "3                   0.439024         0.000000  0.651526  0.223380  0.125093   \n",
       "4                   0.439024         0.000000  0.466919  0.287335  0.245747   \n",
       "...                      ...              ...       ...       ...       ...   \n",
       "4175                0.439024         0.666667  0.254902  0.254902  0.490196   \n",
       "4176                0.414634         0.466667  0.507262  0.276010  0.216728   \n",
       "4177                0.560976         0.600000  0.118960  0.181273  0.699767   \n",
       "4178                0.536585         0.800000  0.263523  0.237171  0.499307   \n",
       "4179                0.317073         0.266667  0.602028  0.198986  0.198986   \n",
       "\n",
       "           BWH  ...  PS_overround  PSC_overround   pH_mean   pD_mean  \\\n",
       "0     0.501789  ...      1.084635       1.084635  0.486191  0.282183   \n",
       "1     0.548686  ...      1.086819       1.086819  0.544034  0.266139   \n",
       "2     0.477612  ...      1.090072       1.090072  0.467183  0.285318   \n",
       "3     0.647450  ...      1.084317       1.084317  0.642133  0.234665   \n",
       "4     0.518664  ...      1.085347       1.085347  0.492309  0.283573   \n",
       "...        ...  ...           ...            ...       ...       ...   \n",
       "4175  0.271493  ...      1.025927       1.026803  0.260182  0.250866   \n",
       "4176  0.511111  ...      1.026614       1.026469  0.501490  0.285331   \n",
       "4177  0.119069  ...      1.032115       1.034061  0.119226  0.192054   \n",
       "4178  0.257662  ...      1.028292       1.027454  0.249476  0.232252   \n",
       "4179  0.608078  ...      1.031156       1.032229  0.622459  0.193352   \n",
       "\n",
       "       pA_mean  overround_mean  overround_std  home_adv  draw_tightness  \\\n",
       "0     0.231626        1.087405       0.042728  0.254565        0.282183   \n",
       "1     0.189827        1.088623       0.037464  0.354208        0.266139   \n",
       "2     0.247500        1.089762       0.033230  0.219683        0.285318   \n",
       "3     0.123202        1.082695       0.041275  0.518931        0.234665   \n",
       "4     0.224118        1.085243       0.041920  0.268191        0.283573   \n",
       "...        ...             ...            ...       ...             ...   \n",
       "4175  0.488952        1.037138       0.016111 -0.228769        0.250866   \n",
       "4176  0.213179        1.040258       0.019152  0.288311        0.285331   \n",
       "4177  0.688720        1.039219       0.014740 -0.569493        0.192054   \n",
       "4178  0.518272        1.040577       0.015316 -0.268796        0.232252   \n",
       "4179  0.184189        1.038654       0.017043  0.438270        0.193352   \n",
       "\n",
       "        elo_diff  \n",
       "0       0.000000  \n",
       "1       0.000000  \n",
       "2       0.000000  \n",
       "3       0.000000  \n",
       "4       0.000000  \n",
       "...          ...  \n",
       "4175 -235.644217  \n",
       "4176    7.004375  \n",
       "4177 -161.814985  \n",
       "4178 -302.934468  \n",
       "4179  190.665160  \n",
       "\n",
       "[4180 rows x 48 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaxH</th>\n",
       "      <th>MaxD</th>\n",
       "      <th>MaxA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.480966</td>\n",
       "      <td>0.295652</td>\n",
       "      <td>0.223382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.563347</td>\n",
       "      <td>0.254087</td>\n",
       "      <td>0.182566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.466712</td>\n",
       "      <td>0.288264</td>\n",
       "      <td>0.245024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.673031</td>\n",
       "      <td>0.214797</td>\n",
       "      <td>0.112172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.480418</td>\n",
       "      <td>0.300261</td>\n",
       "      <td>0.219321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0.263796</td>\n",
       "      <td>0.253650</td>\n",
       "      <td>0.482554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>0.514058</td>\n",
       "      <td>0.283466</td>\n",
       "      <td>0.202476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4177</th>\n",
       "      <td>0.115016</td>\n",
       "      <td>0.188407</td>\n",
       "      <td>0.696577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4178</th>\n",
       "      <td>0.256331</td>\n",
       "      <td>0.234970</td>\n",
       "      <td>0.508699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4179</th>\n",
       "      <td>0.614688</td>\n",
       "      <td>0.195637</td>\n",
       "      <td>0.189675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4180 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          MaxH      MaxD      MaxA\n",
       "0     0.480966  0.295652  0.223382\n",
       "1     0.563347  0.254087  0.182566\n",
       "2     0.466712  0.288264  0.245024\n",
       "3     0.673031  0.214797  0.112172\n",
       "4     0.480418  0.300261  0.219321\n",
       "...        ...       ...       ...\n",
       "4175  0.263796  0.253650  0.482554\n",
       "4176  0.514058  0.283466  0.202476\n",
       "4177  0.115016  0.188407  0.696577\n",
       "4178  0.256331  0.234970  0.508699\n",
       "4179  0.614688  0.195637  0.189675\n",
       "\n",
       "[4180 rows x 3 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.loc[:, [\"MaxH\", \"MaxD\", \"MaxA\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:30px;'>Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Define feature groups ----\n",
    "\n",
    "# Form features\n",
    "form_features = [\n",
    "    \"HomeTeam_points\", \"AwayTeam_points\",\n",
    "    \"HomeTeam_avg_goal_diff\", \"AwayTeam_avg_goal_diff\"\n",
    "]\n",
    "\n",
    "# Raw bookmaker odds (all H/D/A triplets)\n",
    "bookmaker_odds = [\n",
    "    \"B365H\",\"B365D\",\"B365A\",\n",
    "    \"BWH\",\"BWD\",\"BWA\",\n",
    "    \"IWH\",\"IWD\",\"IWA\",\n",
    "    \"WHH\",\"WHD\",\"WHA\",\n",
    "    \"VCH\",\"VCD\",\"VCA\",\n",
    "    \"MaxH\",\"MaxD\",\"MaxA\",\n",
    "    \"PSH\",\"PSD\",\"PSA\",\n",
    "    \"PSCH\",\"PSCD\",\"PSCA\"\n",
    "]\n",
    "\n",
    "# Overrounds (one per bookmaker set)\n",
    "overrounds = [\n",
    "    \"B365_overround\",\"BW_overround\",\"IW_overround\",\"WH_overround\",\n",
    "    \"VC_overround\",\"Max_overround\",\"PS_overround\",\"PSC_overround\"\n",
    "]\n",
    "\n",
    "# Elo ratings\n",
    "elo_features = [\"home_elo\",\"away_elo\",\"elo_diff\"]\n",
    "\n",
    "# Consensus features\n",
    "consensus_features = [\"pH_mean\",\"pD_mean\",\"pA_mean\",\"overround_mean\",\"overround_std\"]\n",
    "\n",
    "# Engineered extras\n",
    "engineered_features = [\"home_adv\",\"draw_tightness\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0) Feature set: odds + form + raw team IDs as categorical ---\n",
    "feat_cats  = [\"HomeTeam\", \"AwayTeam\"]              # keep as strings (object)\n",
    "feat_nums  = form_features        # your existing lists\n",
    "feat_all   = feat_cats + feat_nums + bookmaker_odds\n",
    "\n",
    "Xtr = X_train[feat_all].copy()\n",
    "Xte = X_test[feat_all].copy()\n",
    "ytr, yte = y_train, y_test\n",
    "\n",
    "# Ensure the two categorical cols are string dtype (safer than category)\n",
    "for c in feat_cats:\n",
    "    Xtr[c] = Xtr[c].astype(str)\n",
    "    Xte[c] = Xte[c].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>l2_leaf_reg</th>\n",
       "      <th>random_strength</th>\n",
       "      <th>bagging_temperature</th>\n",
       "      <th>rsm</th>\n",
       "      <th>acc</th>\n",
       "      <th>logloss</th>\n",
       "      <th>c0_rec</th>\n",
       "      <th>c1_rec</th>\n",
       "      <th>c2_rec</th>\n",
       "      <th>c0_f1</th>\n",
       "      <th>c1_f1</th>\n",
       "      <th>c2_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.12</td>\n",
       "      <td>15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.014689</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.439560</td>\n",
       "      <td>0.689266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.10</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.561111</td>\n",
       "      <td>0.997094</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.506024</td>\n",
       "      <td>0.435644</td>\n",
       "      <td>0.659091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.30</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.998774</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.505495</td>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.30</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>1.002298</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.459770</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.674157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.30</td>\n",
       "      <td>15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>1.006687</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.439560</td>\n",
       "      <td>0.644809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.12</td>\n",
       "      <td>20</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>1.015061</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.457831</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.670455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.30</td>\n",
       "      <td>20</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>1.021977</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.494382</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.662857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.30</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>0.987681</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.441860</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.10</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>0.997917</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.684783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.12</td>\n",
       "      <td>15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>0.998953</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.378947</td>\n",
       "      <td>0.655367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10</td>\n",
       "      <td>15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>1.002035</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.487805</td>\n",
       "      <td>0.365591</td>\n",
       "      <td>0.659459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.12</td>\n",
       "      <td>25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>1.007201</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.378947</td>\n",
       "      <td>0.640884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.10</td>\n",
       "      <td>25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>1.010785</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.651934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.30</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>1.015573</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.378947</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.12</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>1.016628</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.651429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     learning_rate  l2_leaf_reg  random_strength  bagging_temperature   rsm  \\\n",
       "39            0.12           15              0.5                  0.5  1.00   \n",
       "11            0.10           15              1.0                  0.9  1.00   \n",
       "90            0.30           20              1.0                  0.3  0.85   \n",
       "105           0.30           25              1.0                  0.5  1.00   \n",
       "74            0.30           15              0.5                  0.5  0.85   \n",
       "51            0.12           20              0.5                  0.5  1.00   \n",
       "84            0.30           20              0.5                  0.3  0.85   \n",
       "80            0.30           15              1.0                  0.5  0.85   \n",
       "32            0.10           25              1.0                  0.5  0.85   \n",
       "38            0.12           15              0.5                  0.5  0.85   \n",
       "0             0.10           15              0.5                  0.3  0.85   \n",
       "64            0.12           25              0.5                  0.9  0.85   \n",
       "24            0.10           25              0.5                  0.3  0.85   \n",
       "78            0.30           15              1.0                  0.3  0.85   \n",
       "69            0.12           25              1.0                  0.5  1.00   \n",
       "\n",
       "          acc   logloss    c0_rec  c1_rec    c2_rec     c0_f1     c1_f1  \\\n",
       "39   0.583333  1.014689  0.510638    0.40  0.734940  0.521739  0.439560   \n",
       "11   0.561111  0.997094  0.446809    0.44  0.698795  0.506024  0.435644   \n",
       "90   0.550000  0.998774  0.489362    0.30  0.734940  0.505495  0.348837   \n",
       "105  0.550000  1.002298  0.425532    0.38  0.722892  0.459770  0.400000   \n",
       "74   0.550000  1.006687  0.425532    0.40  0.710843  0.465116  0.439560   \n",
       "51   0.550000  1.015061  0.404255    0.42  0.710843  0.457831  0.415842   \n",
       "84   0.550000  1.021977  0.468085    0.38  0.698795  0.494382  0.395833   \n",
       "80   0.544444  0.987681  0.404255    0.34  0.746988  0.441860  0.386364   \n",
       "32   0.544444  0.997917  0.404255    0.32  0.759036  0.447059  0.351648   \n",
       "38   0.544444  0.998953  0.468085    0.36  0.698795  0.500000  0.378947   \n",
       "0    0.544444  1.002035  0.425532    0.34  0.734940  0.487805  0.365591   \n",
       "64   0.544444  1.007201  0.468085    0.36  0.698795  0.523810  0.378947   \n",
       "24   0.544444  1.010785  0.446809    0.36  0.710843  0.482759  0.391304   \n",
       "78   0.544444  1.015573  0.489362    0.36  0.686747  0.500000  0.378947   \n",
       "69   0.544444  1.016628  0.446809    0.40  0.686747  0.482759  0.408163   \n",
       "\n",
       "        c2_f1  \n",
       "39   0.689266  \n",
       "11   0.659091  \n",
       "90   0.666667  \n",
       "105  0.674157  \n",
       "74   0.644809  \n",
       "51   0.670455  \n",
       "84   0.662857  \n",
       "80   0.666667  \n",
       "32   0.684783  \n",
       "38   0.655367  \n",
       "0    0.659459  \n",
       "64   0.640884  \n",
       "24   0.651934  \n",
       "78   0.658960  \n",
       "69   0.651429  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feat_cats + feat_nums + bookmaker_odds\n",
    "# --- 1) Param grid (your spec) ---\n",
    "param_grid = {\n",
    "    \"learning_rate\":       [0.10, 0.12, 0.30],\n",
    "    \"l2_leaf_reg\":         [15, 20, 25],\n",
    "    \"random_strength\":     [0.5, 1.0],\n",
    "    \"bagging_temperature\": [0.3, 0.5, 0.9],  # Bayesian bootstrap\n",
    "    \"rsm\":                 [0.85, 1.0],\n",
    "}\n",
    "\n",
    "# --- 2) Helper to iterate a dict grid (readable) ---\n",
    "import itertools, pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, precision_recall_fscore_support\n",
    "\n",
    "def combos(grid):\n",
    "    keys = list(grid.keys())\n",
    "    for vals in itertools.product(*(grid[k] for k in keys)):\n",
    "        yield dict(zip(keys, vals))\n",
    "\n",
    "# --- 3) Run grid (CatBoost with categorical team IDs) ---\n",
    "rows = []\n",
    "for p in combos(param_grid):\n",
    "    model = CatBoostClassifier(\n",
    "        loss_function=\"MultiClass\",\n",
    "        eval_metric=\"Accuracy\",\n",
    "        iterations=1000,\n",
    "        depth=6,                         # keep your common choice; modify if you want\n",
    "        class_weights=[1, 1.5, 1],       # keep your current weighting\n",
    "        verbose=False,\n",
    "        random_state=42,\n",
    "        **p\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        Xtr, ytr,\n",
    "        eval_set=(Xte, yte),\n",
    "        use_best_model=True,\n",
    "        cat_features=feat_cats          # << tell CatBoost to treat these as categorical\n",
    "    )\n",
    "\n",
    "    y_pred  = model.predict(Xte)\n",
    "    y_proba = model.predict_proba(Xte)\n",
    "\n",
    "    # per-class metrics with stable ordering (0=Away, 1=Draw, 2=Home for your labels)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(\n",
    "        yte, y_pred, zero_division=0\n",
    "    )\n",
    "\n",
    "    rows.append({\n",
    "        **p,\n",
    "        \"acc\": accuracy_score(yte, y_pred),\n",
    "        \"logloss\": log_loss(yte, y_proba),\n",
    "        \"c0_rec\": rc[0], \"c1_rec\": rc[1], \"c2_rec\": rc[2],\n",
    "        \"c0_f1\":  f1[0], \"c1_f1\":  f1[1], \"c2_f1\":  f1[2],\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(rows).sort_values([\"acc\",\"logloss\"], ascending=[False, True])\n",
    "display(results.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.58\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.51      0.52        47\n",
      "           1       0.49      0.40      0.44        50\n",
      "           2       0.65      0.73      0.69        83\n",
      "\n",
      "    accuracy                           0.58       180\n",
      "   macro avg       0.56      0.55      0.55       180\n",
      "weighted avg       0.57      0.58      0.58       180\n",
      "\n",
      "{'learn': {'Accuracy': 0.880949834206867, 'MultiClass': 0.5646166049591457}, 'validation': {'Accuracy': 0.5609756097560976, 'MultiClass': 1.0115546897344136}}\n"
     ]
    }
   ],
   "source": [
    "# feat_cats + feat_nums + bookmaker_odds\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "model = CatBoostClassifier(iterations=1000, loss_function=\"MultiClass\", random_strength=0.5, bagging_temperature=0.5, rsm=1,\n",
    "                           eval_metric=\"Accuracy\", learning_rate=0.12, random_state=42,\n",
    "                           l2_leaf_reg=15, class_weights=[1, 1.5, 1])\n",
    "\n",
    "model.fit(Xtr,\n",
    "          ytr,\n",
    "          eval_set=(Xte, yte),\n",
    "          cat_features=feat_cats,    \n",
    "          verbose=False)\n",
    "\n",
    "y_pred = model.predict(Xte)\n",
    "print(f\"Test Set Accuracy: {accuracy_score(yte, y_pred):.2f}\")\n",
    "print(classification_report(yte, y_pred, zero_division=0))\n",
    "print(model.get_best_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>l2_leaf_reg</th>\n",
       "      <th>random_strength</th>\n",
       "      <th>bagging_temperature</th>\n",
       "      <th>rsm</th>\n",
       "      <th>acc</th>\n",
       "      <th>logloss</th>\n",
       "      <th>c0_rec</th>\n",
       "      <th>c1_rec</th>\n",
       "      <th>c2_rec</th>\n",
       "      <th>c0_f1</th>\n",
       "      <th>c1_f1</th>\n",
       "      <th>c2_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.30</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>1.029679</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.655556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.30</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.538889</td>\n",
       "      <td>1.009837</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.469136</td>\n",
       "      <td>0.361702</td>\n",
       "      <td>0.659459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.12</td>\n",
       "      <td>25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.538889</td>\n",
       "      <td>1.011861</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.471910</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.648045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.12</td>\n",
       "      <td>20</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.538889</td>\n",
       "      <td>1.015997</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.635838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.10</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.538889</td>\n",
       "      <td>1.016868</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.351648</td>\n",
       "      <td>0.648352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.30</td>\n",
       "      <td>20</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.538889</td>\n",
       "      <td>1.026381</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.617978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.30</td>\n",
       "      <td>20</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.538889</td>\n",
       "      <td>1.027686</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.365591</td>\n",
       "      <td>0.648045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.12</td>\n",
       "      <td>25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.538889</td>\n",
       "      <td>1.028556</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.527473</td>\n",
       "      <td>0.298851</td>\n",
       "      <td>0.659341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.30</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.538889</td>\n",
       "      <td>1.038728</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>0.651685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.30</td>\n",
       "      <td>25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.538889</td>\n",
       "      <td>1.038742</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.12</td>\n",
       "      <td>20</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.999067</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.457831</td>\n",
       "      <td>0.344086</td>\n",
       "      <td>0.663043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.30</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1.003452</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.457831</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.659459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1.005664</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.361702</td>\n",
       "      <td>0.648045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.12</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1.005957</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.365591</td>\n",
       "      <td>0.640884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.12</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>1.010298</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.651934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     learning_rate  l2_leaf_reg  random_strength  bagging_temperature   rsm  \\\n",
       "78            0.30           15              1.0                  0.3  0.85   \n",
       "105           0.30           25              1.0                  0.5  1.00   \n",
       "65            0.12           25              0.5                  0.9  1.00   \n",
       "50            0.12           20              0.5                  0.5  0.85   \n",
       "7             0.10           15              1.0                  0.3  1.00   \n",
       "85            0.30           20              0.5                  0.3  1.00   \n",
       "87            0.30           20              0.5                  0.5  1.00   \n",
       "62            0.12           25              0.5                  0.5  0.85   \n",
       "93            0.30           20              1.0                  0.5  1.00   \n",
       "100           0.30           25              0.5                  0.9  0.85   \n",
       "48            0.12           20              0.5                  0.3  0.85   \n",
       "102           0.30           25              1.0                  0.3  0.85   \n",
       "2             0.10           15              0.5                  0.5  0.85   \n",
       "56            0.12           20              1.0                  0.5  0.85   \n",
       "42            0.12           15              1.0                  0.3  0.85   \n",
       "\n",
       "          acc   logloss    c0_rec  c1_rec    c2_rec     c0_f1     c1_f1  \\\n",
       "78   0.550000  1.029679  0.425532    0.40  0.710843  0.465116  0.425532   \n",
       "105  0.538889  1.009837  0.404255    0.34  0.734940  0.469136  0.361702   \n",
       "65   0.538889  1.011861  0.446809    0.36  0.698795  0.471910  0.391304   \n",
       "50   0.538889  1.015997  0.468085    0.40  0.662651  0.505747  0.400000   \n",
       "7    0.538889  1.016868  0.468085    0.32  0.710843  0.505747  0.351648   \n",
       "85   0.538889  1.026381  0.468085    0.40  0.662651  0.530120  0.404040   \n",
       "87   0.538889  1.027686  0.468085    0.34  0.698795  0.500000  0.365591   \n",
       "62   0.538889  1.028556  0.510638    0.26  0.722892  0.527473  0.298851   \n",
       "93   0.538889  1.038728  0.468085    0.34  0.698795  0.478261  0.377778   \n",
       "100  0.538889  1.038742  0.446809    0.40  0.674699  0.494118  0.400000   \n",
       "48   0.533333  0.999067  0.404255    0.32  0.734940  0.457831  0.344086   \n",
       "102  0.533333  1.003452  0.404255    0.32  0.734940  0.457831  0.347826   \n",
       "2    0.533333  1.005664  0.446809    0.34  0.698795  0.482759  0.361702   \n",
       "56   0.533333  1.005957  0.446809    0.34  0.698795  0.488372  0.365591   \n",
       "42   0.533333  1.010298  0.382979    0.38  0.710843  0.444444  0.387755   \n",
       "\n",
       "        c2_f1  \n",
       "78   0.655556  \n",
       "105  0.659459  \n",
       "65   0.648045  \n",
       "50   0.635838  \n",
       "7    0.648352  \n",
       "85   0.617978  \n",
       "87   0.648045  \n",
       "62   0.659341  \n",
       "93   0.651685  \n",
       "100  0.640000  \n",
       "48   0.663043  \n",
       "102  0.659459  \n",
       "2    0.648045  \n",
       "56   0.640884  \n",
       "42   0.651934  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feat_cats + feat_nums + bookmaker_odds + elo_features\n",
    "\n",
    "# --- 1) Param grid (your spec) ---\n",
    "param_grid = {\n",
    "    \"learning_rate\":       [0.10, 0.12, 0.30],\n",
    "    \"l2_leaf_reg\":         [15, 20, 25],\n",
    "    \"random_strength\":     [0.5, 1.0],\n",
    "    \"bagging_temperature\": [0.3, 0.5, 0.9],  # Bayesian bootstrap\n",
    "    \"rsm\":                 [0.85, 1.0],\n",
    "}\n",
    "\n",
    "# --- 2) Helper to iterate a dict grid (readable) ---\n",
    "import itertools, pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, precision_recall_fscore_support\n",
    "\n",
    "def combos(grid):\n",
    "    keys = list(grid.keys())\n",
    "    for vals in itertools.product(*(grid[k] for k in keys)):\n",
    "        yield dict(zip(keys, vals))\n",
    "\n",
    "# --- 3) Run grid (CatBoost with categorical team IDs) ---\n",
    "rows = []\n",
    "for p in combos(param_grid):\n",
    "    model = CatBoostClassifier(\n",
    "        loss_function=\"MultiClass\",\n",
    "        eval_metric=\"Accuracy\",\n",
    "        iterations=1000,\n",
    "        depth=6,                         # keep your common choice; modify if you want\n",
    "        class_weights=[1, 1.5, 1],       # keep your current weighting\n",
    "        random_state=42,\n",
    "        verbose=False,\n",
    "        **p\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        Xtr, ytr,\n",
    "        eval_set=(Xte, yte),\n",
    "        use_best_model=True,\n",
    "        cat_features=feat_cats          # << tell CatBoost to treat these as categorical\n",
    "    )\n",
    "\n",
    "    y_pred  = model.predict(Xte)\n",
    "    y_proba = model.predict_proba(Xte)\n",
    "\n",
    "    # per-class metrics with stable ordering (0=Away, 1=Draw, 2=Home for your labels)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(\n",
    "        yte, y_pred, zero_division=0\n",
    "    )\n",
    "\n",
    "    rows.append({\n",
    "        **p,\n",
    "        \"acc\": accuracy_score(yte, y_pred),\n",
    "        \"logloss\": log_loss(yte, y_proba),\n",
    "        \"c0_rec\": rc[0], \"c1_rec\": rc[1], \"c2_rec\": rc[2],\n",
    "        \"c0_f1\":  f1[0], \"c1_f1\":  f1[1], \"c2_f1\":  f1[2],\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(rows).sort_values([\"acc\",\"logloss\"], ascending=[False, True])\n",
    "display(results.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Counter({2: 2013, 0: 1178, 1: 989})\n",
      "After : Counter({2: 2013, 0: 2013, 1: 2013})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from collections import Counter\n",
    "\n",
    "# --- Encode categorical features ---\n",
    "encoders = {}\n",
    "Xtr_enc = Xtr.copy()\n",
    "\n",
    "for c in feat_cats:  # e.g. [\"HomeTeam\", \"AwayTeam\"]\n",
    "    le = LabelEncoder()\n",
    "    Xtr_enc[c] = le.fit_transform(Xtr[c])   # encode training set only\n",
    "    encoders[c] = le\n",
    "\n",
    "# Indices of categorical columns for SMOTENC\n",
    "cat_idx = [Xtr_enc.columns.get_loc(c) for c in feat_cats]\n",
    "\n",
    "# --- Apply SMOTENC ---\n",
    "smote = SMOTENC(\n",
    "    categorical_features=cat_idx,\n",
    "    sampling_strategy=\"not majority\",  # upsample draws & away, keep home\n",
    "    random_state=42\n",
    ")\n",
    "Xtr_bal, ytr_bal = smote.fit_resample(Xtr_enc, ytr)\n",
    "\n",
    "print(\"Before:\", Counter(ytr))\n",
    "print(\"After :\", Counter(ytr_bal))\n",
    "\n",
    "# --- Optional: Decode categorical cols back to original strings ---\n",
    "Xtr_bal = pd.DataFrame(Xtr_bal, columns=Xtr_enc.columns)\n",
    "for c in feat_cats:\n",
    "    Xtr_bal[c] = encoders[c].inverse_transform(Xtr_bal[c].astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>depth</th>\n",
       "      <th>l2_leaf_reg</th>\n",
       "      <th>random_strength</th>\n",
       "      <th>bagging_temperature</th>\n",
       "      <th>rsm</th>\n",
       "      <th>grow_policy</th>\n",
       "      <th>acc</th>\n",
       "      <th>logloss</th>\n",
       "      <th>c0_rec</th>\n",
       "      <th>c1_rec</th>\n",
       "      <th>c2_rec</th>\n",
       "      <th>c0_f1</th>\n",
       "      <th>c1_f1</th>\n",
       "      <th>c2_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.982381</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.681818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.15</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.572222</td>\n",
       "      <td>1.010951</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.404494</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.05</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>1.000367</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.534653</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.561111</td>\n",
       "      <td>1.001364</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.662857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.561111</td>\n",
       "      <td>1.013455</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.651429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.561111</td>\n",
       "      <td>1.014537</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.568421</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.662983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0.15</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.561111</td>\n",
       "      <td>1.025662</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.367816</td>\n",
       "      <td>0.674286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.15</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.561111</td>\n",
       "      <td>1.044534</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>0.674157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.998675</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.536082</td>\n",
       "      <td>0.373626</td>\n",
       "      <td>0.662791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.008282</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.337349</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0.10</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.008302</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>0.337079</td>\n",
       "      <td>0.662791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.008381</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.367816</td>\n",
       "      <td>0.662921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.010229</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.547368</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0.15</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.015653</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.505263</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.674033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.015664</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     learning_rate  depth  l2_leaf_reg  random_strength  bagging_temperature  \\\n",
       "313           0.15      6            5              2.0                  0.5   \n",
       "397           0.15      8            3              2.0                  1.0   \n",
       "113           0.05      8            5              1.0                  0.5   \n",
       "171           0.10      6            5              2.0                  0.5   \n",
       "41            0.05      6           10              2.0                  0.5   \n",
       "339           0.15      7            3              1.0                  0.5   \n",
       "393           0.15      8            3              2.0                  0.5   \n",
       "401           0.15      8            5              1.0                  0.5   \n",
       "169           0.10      6            5              2.0                  0.5   \n",
       "151           0.10      6            3              1.0                  1.0   \n",
       "257           0.10      8            5              1.0                  0.5   \n",
       "25            0.05      6            5              2.0                  0.5   \n",
       "147           0.10      6            3              1.0                  0.5   \n",
       "415           0.15      8            5              2.0                  1.0   \n",
       "27            0.05      6            5              2.0                  0.5   \n",
       "\n",
       "     rsm grow_policy       acc   logloss    c0_rec  c1_rec    c2_rec  \\\n",
       "313  0.9   Lossguide  0.577778  0.982381  0.617021    0.30  0.722892   \n",
       "397  0.9   Lossguide  0.572222  1.010951  0.531915    0.36  0.722892   \n",
       "113  0.9   Lossguide  0.566667  1.000367  0.574468    0.30  0.722892   \n",
       "171  1.0   Lossguide  0.561111  1.001364  0.595745    0.30  0.698795   \n",
       "41   0.9   Lossguide  0.561111  1.013455  0.595745    0.32  0.686747   \n",
       "339  1.0   Lossguide  0.561111  1.014537  0.574468    0.28  0.722892   \n",
       "393  0.9   Lossguide  0.561111  1.025662  0.553191    0.32  0.710843   \n",
       "401  0.9   Lossguide  0.561111  1.044534  0.510638    0.34  0.722892   \n",
       "169  0.9   Lossguide  0.555556  0.998675  0.553191    0.34  0.686747   \n",
       "151  1.0   Lossguide  0.555556  1.008282  0.574468    0.28  0.710843   \n",
       "257  0.9   Lossguide  0.555556  1.008302  0.595745    0.30  0.686747   \n",
       "25   0.9   Lossguide  0.555556  1.008381  0.531915    0.32  0.710843   \n",
       "147  1.0   Lossguide  0.555556  1.010229  0.553191    0.34  0.686747   \n",
       "415  1.0   Lossguide  0.555556  1.015653  0.510638    0.30  0.734940   \n",
       "27   1.0   Lossguide  0.555556  1.015664  0.531915    0.34  0.698795   \n",
       "\n",
       "        c0_f1     c1_f1     c2_f1  \n",
       "313  0.568627  0.365854  0.681818  \n",
       "397  0.520833  0.404494  0.685714  \n",
       "113  0.534653  0.357143  0.685714  \n",
       "171  0.571429  0.344828  0.662857  \n",
       "41   0.565657  0.372093  0.651429  \n",
       "339  0.568421  0.333333  0.662983  \n",
       "393  0.530612  0.367816  0.674286  \n",
       "401  0.510638  0.386364  0.674157  \n",
       "169  0.536082  0.373626  0.662791  \n",
       "151  0.540000  0.337349  0.666667  \n",
       "257  0.565657  0.337079  0.662791  \n",
       "25   0.526316  0.367816  0.662921  \n",
       "147  0.547368  0.369565  0.658960  \n",
       "415  0.505263  0.357143  0.674033  \n",
       "27   0.531915  0.369565  0.666667  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feat_cats + feat_nums + bookmaker_odds + elo_features\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.05, 0.1, 0.15],\n",
    "    \"depth\": [6, 7, 8],\n",
    "    \"l2_leaf_reg\": [3, 5, 10],\n",
    "    \"random_strength\": [1.0, 2.0],\n",
    "    \"bagging_temperature\": [0.5, 1.0],\n",
    "    \"rsm\": [0.9, 1.0],\n",
    "    \"grow_policy\": [\"SymmetricTree\", \"Lossguide\"],  # compare both\n",
    "}\n",
    "\n",
    "# --- 2) Helper to iterate a dict grid (readable) ---\n",
    "import itertools, pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, precision_recall_fscore_support\n",
    "\n",
    "def combos(grid):\n",
    "    keys = list(grid.keys())\n",
    "    for vals in itertools.product(*(grid[k] for k in keys)):\n",
    "        yield dict(zip(keys, vals))\n",
    "\n",
    "# --- 3) Run grid (CatBoost with categorical team IDs) ---\n",
    "rows = []\n",
    "from sklearn.utils import shuffle\n",
    "Xtr_bal, ytr_bal = shuffle(Xtr_bal, ytr_bal, random_state=42)\n",
    "\n",
    "for p in combos(param_grid):\n",
    "    model = CatBoostClassifier(\n",
    "        loss_function=\"MultiClass\",\n",
    "        eval_metric=\"Accuracy\",\n",
    "        iterations=1000,\n",
    "        # class_weights=[1, 1.5, 1],       # keep your current weighting\n",
    "        verbose=False,\n",
    "        random_state=42,\n",
    "        **p\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        Xtr_bal, ytr_bal,\n",
    "        eval_set=(Xte, yte),\n",
    "        use_best_model=True,\n",
    "        cat_features=feat_cats          # << tell CatBoost to treat these as categorical\n",
    "    )\n",
    "\n",
    "    y_pred  = model.predict(Xte)\n",
    "    y_proba = model.predict_proba(Xte)\n",
    "\n",
    "    # per-class metrics with stable ordering (0=Away, 1=Draw, 2=Home for your labels)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(\n",
    "        yte, y_pred, zero_division=0\n",
    "    )\n",
    "\n",
    "    rows.append({\n",
    "        **p,\n",
    "        \"acc\": accuracy_score(yte, y_pred),\n",
    "        \"logloss\": log_loss(yte, y_proba),\n",
    "        \"c0_rec\": rc[0], \"c1_rec\": rc[1], \"c2_rec\": rc[2],\n",
    "        \"c0_f1\":  f1[0], \"c1_f1\":  f1[1], \"c2_f1\":  f1[2],\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(rows).sort_values([\"acc\",\"logloss\"], ascending=[False, True])\n",
    "display(results.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>depth</th>\n",
       "      <th>l2_leaf_reg</th>\n",
       "      <th>random_strength</th>\n",
       "      <th>bagging_temperature</th>\n",
       "      <th>rsm</th>\n",
       "      <th>grow_policy</th>\n",
       "      <th>acc</th>\n",
       "      <th>logloss</th>\n",
       "      <th>c0_rec</th>\n",
       "      <th>c1_rec</th>\n",
       "      <th>c2_rec</th>\n",
       "      <th>c0_f1</th>\n",
       "      <th>c1_f1</th>\n",
       "      <th>c2_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.594444</td>\n",
       "      <td>1.027716</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.704545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.05</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>1.000995</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.547368</td>\n",
       "      <td>0.436782</td>\n",
       "      <td>0.685393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>0.10</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>1.004595</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.505263</td>\n",
       "      <td>0.483516</td>\n",
       "      <td>0.689655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>1.005583</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.568421</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.678363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>1.006023</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.681818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.993366</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.685714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.005941</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.483516</td>\n",
       "      <td>0.677966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0.10</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.006859</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.682927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0.15</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.012315</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.556701</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.678363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.015563</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>0.674556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.10</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.022062</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.698795</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.449438</td>\n",
       "      <td>0.674419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>1.002272</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>0.15</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>1.003718</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.556701</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.650888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.10</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>1.003846</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.686747</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.449438</td>\n",
       "      <td>0.662791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Lossguide</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>1.006775</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.549451</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.662722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     learning_rate  depth  l2_leaf_reg  random_strength  bagging_temperature  \\\n",
       "147           0.10      6            3              1.0                  0.5   \n",
       "57            0.05      7            3              2.0                  0.5   \n",
       "275           0.10      8           10              1.0                  0.5   \n",
       "163           0.10      6            5              1.0                  0.5   \n",
       "241           0.10      8            3              1.0                  0.5   \n",
       "303           0.15      6            3              2.0                  1.0   \n",
       "1             0.05      6            3              1.0                  0.5   \n",
       "283           0.10      8           10              2.0                  0.5   \n",
       "293           0.15      6            3              1.0                  1.0   \n",
       "7             0.05      6            3              1.0                  1.0   \n",
       "261           0.10      8            5              1.0                  1.0   \n",
       "155           0.10      6            3              2.0                  0.5   \n",
       "349           0.15      7            3              2.0                  1.0   \n",
       "281           0.10      8           10              2.0                  0.5   \n",
       "13            0.05      6            3              2.0                  1.0   \n",
       "\n",
       "     rsm grow_policy       acc   logloss    c0_rec  c1_rec    c2_rec  \\\n",
       "147  1.0   Lossguide  0.594444  1.027716  0.510638    0.42  0.746988   \n",
       "57   0.9   Lossguide  0.588889  1.000995  0.553191    0.38  0.734940   \n",
       "275  1.0   Lossguide  0.588889  1.004595  0.510638    0.44  0.722892   \n",
       "163  1.0   Lossguide  0.588889  1.005583  0.574468    0.42  0.698795   \n",
       "241  0.9   Lossguide  0.588889  1.006023  0.553191    0.40  0.722892   \n",
       "303  1.0   Lossguide  0.583333  0.993366  0.510638    0.42  0.722892   \n",
       "1    0.9   Lossguide  0.583333  1.005941  0.489362    0.44  0.722892   \n",
       "283  1.0   Lossguide  0.583333  1.006859  0.574468    0.44  0.674699   \n",
       "293  0.9   Lossguide  0.583333  1.012315  0.574468    0.40  0.698795   \n",
       "7    1.0   Lossguide  0.583333  1.015563  0.553191    0.44  0.686747   \n",
       "261  0.9   Lossguide  0.583333  1.022062  0.574468    0.40  0.698795   \n",
       "155  1.0   Lossguide  0.577778  1.002272  0.595745    0.40  0.674699   \n",
       "349  0.9   Lossguide  0.577778  1.003718  0.574468    0.44  0.662651   \n",
       "281  0.9   Lossguide  0.577778  1.003846  0.574468    0.40  0.686747   \n",
       "13   0.9   Lossguide  0.577778  1.006775  0.531915    0.46  0.674699   \n",
       "\n",
       "        c0_f1     c1_f1     c2_f1  \n",
       "147  0.510638  0.466667  0.704545  \n",
       "57   0.547368  0.436782  0.685393  \n",
       "275  0.505263  0.483516  0.689655  \n",
       "163  0.568421  0.446809  0.678363  \n",
       "241  0.541667  0.454545  0.681818  \n",
       "303  0.516129  0.456522  0.685714  \n",
       "1    0.500000  0.483516  0.677966  \n",
       "283  0.551020  0.448980  0.682927  \n",
       "293  0.556701  0.434783  0.678363  \n",
       "7    0.541667  0.463158  0.674556  \n",
       "261  0.545455  0.449438  0.674419  \n",
       "155  0.571429  0.425532  0.666667  \n",
       "349  0.556701  0.468085  0.650888  \n",
       "281  0.545455  0.449438  0.662791  \n",
       "13   0.549451  0.460000  0.662722  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feat_cats + feat_nums + bookmaker_odds\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.05, 0.1, 0.15],\n",
    "    \"depth\": [6, 7, 8],\n",
    "    \"l2_leaf_reg\": [3, 5, 10],\n",
    "    \"random_strength\": [1.0, 2.0],\n",
    "    \"bagging_temperature\": [0.5, 1.0],\n",
    "    \"rsm\": [0.9, 1.0],\n",
    "    \"grow_policy\": [\"SymmetricTree\", \"Lossguide\"],  # compare both\n",
    "}\n",
    "\n",
    "# --- 2) Helper to iterate a dict grid (readable) ---\n",
    "import itertools, pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss, precision_recall_fscore_support\n",
    "\n",
    "def combos(grid):\n",
    "    keys = list(grid.keys())\n",
    "    for vals in itertools.product(*(grid[k] for k in keys)):\n",
    "        yield dict(zip(keys, vals))\n",
    "\n",
    "# --- 3) Run grid (CatBoost with categorical team IDs) ---\n",
    "rows = []\n",
    "from sklearn.utils import shuffle\n",
    "Xtr_bal, ytr_bal = shuffle(Xtr_bal, ytr_bal, random_state=42)\n",
    "\n",
    "for p in combos(param_grid):\n",
    "    model = CatBoostClassifier(\n",
    "        loss_function=\"MultiClass\",\n",
    "        eval_metric=\"Accuracy\",\n",
    "        iterations=1000,\n",
    "        # class_weights=[1, 1.5, 1],       # keep your current weighting\n",
    "        verbose=False,\n",
    "        random_state=42,\n",
    "        **p\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        Xtr_bal, ytr_bal,\n",
    "        eval_set=(Xte, yte),\n",
    "        use_best_model=True,\n",
    "        cat_features=feat_cats          # << tell CatBoost to treat these as categorical\n",
    "    )\n",
    "\n",
    "    y_pred  = model.predict(Xte)\n",
    "    y_proba = model.predict_proba(Xte)\n",
    "\n",
    "    # per-class metrics with stable ordering (0=Away, 1=Draw, 2=Home for your labels)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(\n",
    "        yte, y_pred, zero_division=0\n",
    "    )\n",
    "\n",
    "    rows.append({\n",
    "        **p,\n",
    "        \"acc\": accuracy_score(yte, y_pred),\n",
    "        \"logloss\": log_loss(yte, y_proba),\n",
    "        \"c0_rec\": rc[0], \"c1_rec\": rc[1], \"c2_rec\": rc[2],\n",
    "        \"c0_f1\":  f1[0], \"c1_f1\":  f1[1], \"c2_f1\":  f1[2],\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(rows).sort_values([\"acc\",\"logloss\"], ascending=[False, True])\n",
    "display(results.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.59\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.51      0.51        47\n",
      "           1       0.53      0.42      0.47        50\n",
      "           2       0.67      0.75      0.70        83\n",
      "\n",
      "    accuracy                           0.59       180\n",
      "   macro avg       0.57      0.56      0.56       180\n",
      "weighted avg       0.59      0.59      0.59       180\n",
      "\n",
      "{'learn': {'Accuracy': 0.9769829441960589, 'MultiClass': 0.3006730253169239}, 'validation': {'Accuracy': 0.5944444444444444, 'MultiClass': 1.00171692944458}}\n"
     ]
    }
   ],
   "source": [
    "# feat_cats + feat_nums + bookmaker_odds\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "model = CatBoostClassifier(iterations=1000, loss_function=\"MultiClass\", random_strength=1, bagging_temperature=0.5, rsm=1,\n",
    "                           eval_metric=\"Accuracy\", learning_rate=0.1, random_state=42, depth=6, grow_policy=\"Lossguide\",\n",
    "                           l2_leaf_reg=3)\n",
    "\n",
    "model.fit(Xtr_bal,\n",
    "          ytr_bal,\n",
    "          eval_set=(Xte, yte),\n",
    "          cat_features=feat_cats,    \n",
    "          verbose=False)\n",
    "\n",
    "y_pred = model.predict(Xte)\n",
    "print(f\"Test Set Accuracy: {accuracy_score(yte, y_pred):.2f}\")\n",
    "print(classification_report(yte, y_pred, zero_division=0))\n",
    "print(model.get_best_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, precision_score\n",
    "\n",
    "def run_model(classifier, param_grid, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, scoring='balanced_accuracy',\n",
    "                               cv=5, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    model = grid_search.best_estimator_\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:30px;'>Random Forest Classifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Test Set Accuracy: 0.51\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.40      0.42        47\n",
      "           1       0.38      0.18      0.24        50\n",
      "           2       0.56      0.76      0.64        83\n",
      "\n",
      "    accuracy                           0.51       180\n",
      "   macro avg       0.46      0.45      0.44       180\n",
      "weighted avg       0.48      0.51      0.47       180\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    }\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "best_params_rf = run_model(classifier, param_grid, X_train, y_train, X_test, y_test)\n",
    "best_params_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Test Set Accuracy: 0.52\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.36      0.39        47\n",
      "           1       0.48      0.24      0.32        50\n",
      "           2       0.57      0.78      0.66        83\n",
      "\n",
      "    accuracy                           0.52       180\n",
      "   macro avg       0.49      0.46      0.46       180\n",
      "weighted avg       0.50      0.52      0.49       180\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 500}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    }\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "best_params_rf = run_model(classifier, param_grid, X_train, y_train, X_test, y_test)\n",
    "best_params_rf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:30px;'>Gradient Boosting Classifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.38      0.40        47\n",
      "           1       0.38      0.20      0.26        50\n",
      "           2       0.56      0.75      0.64        83\n",
      "\n",
      "    accuracy                           0.50       180\n",
      "   macro avg       0.45      0.44      0.43       180\n",
      "weighted avg       0.47      0.50      0.47       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=1000, max_depth=10)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:30px;'>Naive Bayes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Test Set Accuracy: 0.49\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.55      0.50        47\n",
      "           1       0.34      0.46      0.39        50\n",
      "           2       0.70      0.47      0.56        83\n",
      "\n",
      "    accuracy                           0.49       180\n",
      "   macro avg       0.50      0.49      0.48       180\n",
      "weighted avg       0.54      0.49      0.50       180\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'priors': [0.3, 0.4, 0.3], 'var_smoothing': 2.1544346900318868e-11}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "param_grid = {\n",
    "    'var_smoothing': np.logspace(0, -12, num=10),\n",
    "    'priors': [[0.3, 0.4, 0.3]]\n",
    "}\n",
    "classifier = GaussianNB()\n",
    "\n",
    "best_params_nb = run_model(classifier, param_grid, X_train, y_train, X_test, y_test)\n",
    "best_params_nb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:30px;'>Stacking Classifier</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:15px;'>By Using Stacking Classifier, we can have a more balanced result which have a better performance in predicting results for Draw</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.53\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.62      0.53        47\n",
      "           1       0.42      0.30      0.35        50\n",
      "           2       0.64      0.63      0.63        83\n",
      "\n",
      "    accuracy                           0.53       180\n",
      "   macro avg       0.51      0.51      0.50       180\n",
      "weighted avg       0.53      0.53      0.53       180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "\n",
    "classifier_1 = GradientBoostingClassifier(n_estimators=1000, criterion='friedman_mse', learning_rate=0.1, subsample=0.5)\n",
    "classifier_2 = RandomForestClassifier(n_estimators=1000, min_samples_leaf=1, max_leaf_nodes=5)\n",
    "classifier_3 = GaussianNB(var_smoothing=1e-09)\n",
    "sclf = StackingClassifier(estimators = [('rf', classifier_2), ('gb', classifier_1), ('gnb', classifier_3)],\n",
    "                          final_estimator = classifier_3\n",
    "                          )\n",
    "\n",
    "model = sclf.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:30px;'>CatBoost</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "X_test_resampled, y_test_resampled = smote.fit_resample(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.56\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.47      0.50        47\n",
      "           1       0.46      0.36      0.40        50\n",
      "           2       0.60      0.72      0.66        83\n",
      "\n",
      "    accuracy                           0.56       180\n",
      "   macro avg       0.53      0.52      0.52       180\n",
      "weighted avg       0.54      0.56      0.55       180\n",
      "\n",
      "{'learn': {'Accuracy': 0.9836346133276286, 'MultiClass': 0.2857588260274736}, 'validation': {'Accuracy': 0.5317073170731708, 'MultiClass': 1.013489871666235}}\n"
     ]
    }
   ],
   "source": [
    "# New\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "model = CatBoostClassifier(iterations=1000, loss_function=\"MultiClass\", \n",
    "                                eval_metric=\"Accuracy\", learning_rate=0.3, l2_leaf_reg=9, class_weights=[1, 1.5, 1])\n",
    "\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          eval_set=(X_test, y_test),\n",
    "          verbose=False)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "print(model.get_best_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.58\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.45      0.51        47\n",
      "           1       0.50      0.46      0.48        50\n",
      "           2       0.61      0.72      0.66        83\n",
      "\n",
      "    accuracy                           0.58       180\n",
      "   macro avg       0.57      0.54      0.55       180\n",
      "weighted avg       0.58      0.58      0.57       180\n",
      "\n",
      "{'learn': {'Accuracy': 0.9829928334581238, 'MultiClass': 0.2874334984834144}, 'validation': {'Accuracy': 0.5634146341463414, 'MultiClass': 1.0087578860801056}}\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "model = CatBoostClassifier(iterations=1000, loss_function=\"MultiClass\", \n",
    "                                eval_metric=\"Accuracy\", learning_rate=0.3, l2_leaf_reg=9, class_weights=[1, 1.5, 1])\n",
    "\n",
    "model.fit(X_train,\n",
    "          y_train,\n",
    "          eval_set=(X_test, y_test),\n",
    "          verbose=False)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "print(model.get_best_score())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:30px;'>XGBoost</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Test Set Accuracy: 0.52\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.43      0.45        47\n",
      "           1       0.37      0.20      0.26        50\n",
      "           2       0.57      0.76      0.65        83\n",
      "\n",
      "    accuracy                           0.52       180\n",
      "   macro avg       0.47      0.46      0.45       180\n",
      "weighted avg       0.49      0.52      0.49       180\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1,\n",
       " 'max_depth': 3,\n",
       " 'min_child_weight': 5,\n",
       " 'n_estimators': 1000,\n",
       " 'objective': 'multi:softmax'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "grid_params = {\n",
    "    'max_depth': [3,6,9],\n",
    "    'min_child_weight': [1,3,5],\n",
    "    'learning_rate': [0.1, 0.5, 1],\n",
    "    'objective': ['multi:softmax'],\n",
    "    'n_estimators': [1000]\n",
    "}\n",
    "\n",
    "classifier = XGBClassifier()\n",
    "best_params_gb = run_model(classifier, grid_params, X_train, y_train, X_test, y_test)\n",
    "best_params_gb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:30px;'>Save Model</h1>\n",
    "\n",
    "# <h1 style='font-size:15px;'>Catboost is the most efficient model such that it has best balanced prediction result in all 3 possible outcomes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "catboost = CatBoostClassifier(iterations=1000, loss_function=\"MultiClass\", \n",
    "                                eval_metric=\"Accuracy\", learning_rate=0.3, l2_leaf_reg=9, class_weights=[1, 1.5, 1])\n",
    "\n",
    "catboost.fit(X_train,\n",
    "          y_train,\n",
    "          eval_set=(X_test, y_test),\n",
    "          verbose=False)\n",
    "\n",
    "pickle.dump(catboost, open('catboost.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
